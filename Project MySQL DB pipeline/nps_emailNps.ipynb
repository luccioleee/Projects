{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, select, MetaData, Table\n",
    "import requests\n",
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "from datetime import date, datetime, timedelta\n",
    "from threading import Thread\n",
    "\n",
    "from sql_queries import sql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets connections for AzureDB\n",
    "def getConnforMYSQL(f_data, accessType):\n",
    "    list_dialects = pyodbc.drivers()\n",
    "    \n",
    "    for dialect in list_dialects:\n",
    "        try:\n",
    "            server = f_data[accessType][\"server\"]\n",
    "            db = f_data[accessType][\"database\"]\n",
    "            uid = f_data[accessType][\"uid\"]\n",
    "            pwd = f_data[accessType][\"pwd\"]\n",
    "            driver = f_data[accessType][\"dialect_driver\"]\n",
    "            port = f_data[accessType][\"port\"]\n",
    "\n",
    "            if accessType == \"azureAccess\":\n",
    "                if dialect in f_data[accessType][\"list_workingDialects\"]:\n",
    "                    print (f\"trying the dialect: {dialect}\")\n",
    "\n",
    "                    connection_string = (\n",
    "                        \" Driver={%s}\" %dialect +\n",
    "                        \"; SERVER=%s\" %server + \n",
    "                        \"; Database=%s \" %db + \n",
    "                        \"; UID=%s\" %uid +\n",
    "                        \"; PWD=%s\" %pwd\n",
    "                    )\n",
    "                    \n",
    "                    quoted = urllib.parse.quote_plus(connection_string)\n",
    "                    quoted = f_data[accessType][\"dialect_driver\"] + quoted\n",
    "                    #engine = create_engine(quoted, fast_executemany=True).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "                    engine = create_engine(quoted, fast_executemany=True).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "\n",
    "                    print (f\"engine created with dialect = {dialect}\")\n",
    "                    try:\n",
    "                        with engine.begin() as conn:\n",
    "                            df = pd.DataFrame([1], columns = ['test'])\n",
    "                            df.to_sql(\"connectionTestTable\", conn, if_exists=\"replace\", index = False)\n",
    "                            print(f\"engine test sucessful\")\n",
    "                            break\n",
    "                    except:\n",
    "                        print(f\"the dialect = {dialect} didn't work\")\n",
    "            else:\n",
    "                quoted = driver + uid + \":\" + pwd + \"@\" + server + \":\" + str(port) + \"/\" + db\n",
    "                engine = create_engine(quoted).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "            str_error = None\n",
    "\n",
    "        except:\n",
    "            print('exception found, trying other dialect')\n",
    "            pass\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get response from API\n",
    "def setupAPIrequest(utilities, extraParams):\n",
    "    '''\n",
    "    utilities: the utilies file\n",
    "    extraParams: extraParams as Dictionary for adding params in the request\n",
    "    '''\n",
    "    schemeHTTP = utilities[\"HTTP\"][\"schemeHTTP\"]\n",
    "    baseHTTP = utilities[\"HTTP\"][\"baseHTTP\"]\n",
    "    extraHTTP = utilities[\"HTTP\"][\"extraHTTP\"]\n",
    "    headers = utilities[\"HTTP\"][\"headers\"]\n",
    "    \n",
    "    #adds default headers\n",
    "    headers['Accept'] =  \"application/json\"\n",
    "    headers['Content-Type'] =  \"application/json\"   \n",
    "\n",
    "    #check if there is params variables:\n",
    "    paramsHTTP = \"\"\n",
    "    for key, value in utilities[\"HTTP\"].items():\n",
    "        if key == \"params\":\n",
    "            for key, value in utilities[\"HTTP\"][\"params\"].items():\n",
    "                paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "            paramsHTTP = \"?\" + paramsHTTP\n",
    "    if extraParams != \"\":\n",
    "        for key, value in extraParams.items():\n",
    "            paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "        paramsHTTP = paramsHTTP[:-1]\n",
    "    completeHTTP = schemeHTTP + baseHTTP + extraHTTP + paramsHTTP\n",
    "    \n",
    "    if utilities[\"HTTP\"][\"method\"] == \"get\":\n",
    "        response = requests.get(completeHTTP, headers=headers)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeSQL(conn, sql_text):\n",
    "    '''\n",
    "    gets an AzureDB connection and a SQL code to run on the engine\n",
    "    Returns the result query\n",
    "    '''\n",
    "\n",
    "    query_answer = conn.execute(sql_text)\n",
    "     \n",
    "    return query_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorHandle(errSeverity, errReason, additionalInfo, file, engine_azure):\n",
    "    '''\n",
    "    Handles error for logging in AzureDB:\n",
    "    errLocation should be: where is running, application that is running + file name, other info\n",
    "    errDescription should be: what went wrong probably\n",
    "    errProcedure should be: how to restart/check the schedule or other info + if it's ok to retry anytime\n",
    "    errSeverity: 1 to 5, where 1 is wait for next try and 5 is check immediately\n",
    "    the connection is the connection for the AzureDB\n",
    "    '''\n",
    "    print(\"started errorHandle\")\n",
    "\n",
    "    errProcedure = globals()['util'][\"errorSuggestedProcedure\"][errReason]\n",
    "    if additionalInfo != None:\n",
    "        errDescription = globals()['util'][\"errorDescription\"][errReason]\n",
    "    else:\n",
    "        errDescription = additionalInfo\n",
    "\n",
    "    errLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    errRunFileName = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    errRetry = globals()[\"util\"][file][\"nfo\"][\"retryOption\"]\n",
    "\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_errorLogTable (errorDescription, errorProcedure, errorStartTime, errorLocation, errorRetry, errorDuration, errorSeverity)\n",
    "        VALUES ('{errDescription}', '{errProcedure}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{errLocation}: {errRunFileName}', '{errRetry}', {timeDifference.total_seconds()}, {errSeverity}) \n",
    "    \"\"\"\n",
    "    #tabela = Table('nfo_errorLogTable', MetaData(), autoload_with=engine_azure)\n",
    "    #query = sa.insert(tabela).values(errorDescription = errDescription, errorProcedure = errProcedure, errorTime = datetime.now().strftime(\"%d/%m/%Y, %H:%M\"), errorLocation = errLocation, errorSeverity = errSeverity)\n",
    "    \n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the Success outputTable in AzureDB -> wms_vEstoqueConsultaSuccess\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started successHandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"nfo\"][\"runOutputSuccessTable\"]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "\n",
    "    #comes with insertion\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    \n",
    "    #should be changed to attention Len instead of time\n",
    "    globals()['attentionInsertTime'] = datetime.now()\n",
    "    globals()['attentionEndTime'] = datetime.now()\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    \n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    if globals()['util'][file][\"nfo\"][\"hasIdentifier\"] == \"y\":\n",
    "        sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo, identifier, identifierValue)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}', \n",
    "        '{globals()['util'][file][\"nfo\"][\"identifier\"]}' ,{globals()[\"max_identifiervalue\"]}) \n",
    "        \"\"\"\n",
    "\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the attention outputTable in AzureDB -> wms_vEstoqueConsultaAttention\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started attentionhandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"resultAttentionTable\"][file]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    \n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    \n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_attentionTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runInputLocation}', '{runQueryName}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber} , {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()},'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fCorrectTypes(dataFrame, columnsTypes_dict, list_dfAttention):\n",
    "    '''\n",
    "    gets a normalized data frame and a list of columns in a dictionary to change column type on the dataFrame\n",
    "    returns a list_dfAttention a list with datetime errors, dataframe with the altered columns \n",
    "    '''\n",
    "    for column in dataFrame:\n",
    "        for key, value in columnsTypes_dict.items():\n",
    "            if column == key:\n",
    "                data_type = value[\"type\"]\n",
    "                data_format = value[\"format\"]\n",
    "                #copy the df to errDataTime\n",
    "                errDataFrame = dataFrame\n",
    "\n",
    "                #remove empty column cells\n",
    "                errDataFrame = errDataFrame[errDataFrame[column].astype(bool)]\n",
    "                #reindex the errDateTime to match with mask\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                #create a mask where the convertion to datetime fails\n",
    "                if data_type == \"to_dateTime\":\n",
    "                    mask = pd.to_datetime(errDataFrame[column], format=data_format, errors='coerce').isna()\n",
    "                if data_type == \"to_numeric\":\n",
    "                    mask = pd.to_numeric(errDataFrame[column], errors='coerce').isna()\n",
    "\n",
    "                #apply to df the mask from the substitution\n",
    "                errDataFrame = errDataFrame[mask]\n",
    "\n",
    "                #reindex the errDatetime\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                #append dataframe to be concatenated after only if there is > 1 row in the df\n",
    "                if len(errDataFrame) > 0:\n",
    "                    list_dfAttention.append(errDataFrame)\n",
    "\n",
    "                #the main Dataframe is kept with all the data (and the errors are coerced)\n",
    "                if data_type ==  \"to_dateTime\":\n",
    "                    dataFrame[column] = pd.to_datetime(dataFrame[column], format=data_format, errors=\"coerce\")\n",
    "                if data_type == \"to_numeric\":\n",
    "                    #remove commas in case the numbers are stored as string\n",
    "                    dataFrame[column] = dataFrame[column].replace(regex = {'[A-Za-z/.]', ''}).replace(regex = {',', '.'})\n",
    "                    #change dType\n",
    "                    dataFrame[column] = pd.to_numeric(dataFrame[column], errors='coerce')\n",
    "                break\n",
    "        if dataFrame[column].dtype == int or dataFrame[column].dtype == float :\n",
    "            dataFrame[column].fillna(0, inplace=True)\n",
    "        else:\n",
    "            dataFrame[column].fillna(\"\", inplace=True)\n",
    "                \n",
    "    \n",
    "    return dataFrame, list_dfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class threadMain:\n",
    "    def __init__(self, functionName, queryName, identifier, identifierValue, file, engine_query, engine_azure):\n",
    "        #set variables\n",
    "        self.identifier = identifier\n",
    "        self.identifierValue = identifierValue\n",
    "        self.functionName = functionName\n",
    "        self.queryName = queryName\n",
    "        self.engine_query = engine_query\n",
    "        self.engine_azure = engine_azure\n",
    "        self.file = file\n",
    "        #start the thread\n",
    "        self.t = Thread(target=self.threadMainTask, args=())\n",
    "        self.t.start()\n",
    "\n",
    "    def getThread(self):\n",
    "        return (self.t)\n",
    "    \n",
    "    def threadMainTask(self):\n",
    "        print(f'starting {self.functionName} @{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "        \n",
    "        #get data from npsDB\n",
    "        output = executeSQL(self.engine_query, sql_list[self.queryName] %self.identifierValue)\n",
    "        \n",
    "        #start a dataframe to insert in temp_tables\n",
    "\n",
    "        list_dfAttention = []\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(output)\n",
    "            if len(df) > 0:\n",
    "                #start data-formatting\n",
    "                print ('starting dataType formatting')\n",
    "                df, list_dfAttention = fCorrectTypes(df, globals()['util'][self.file][\"columnsType_dict\"][self.functionName], list_dfAttention)\n",
    "\n",
    "                #include the rows in a sigle dataframe then remove duplicates\n",
    "                if len(list_dfAttention) > 0:\n",
    "                    df_attention = pd.concat(list_dfAttention, ignore_index=True)\n",
    "                    df_attention = df_attention.drop_duplicates()\n",
    "                    #adds the dataconsulta column for historic registration of the error\n",
    "                    df_attention['dataconsulta'] = pd.Timestamp.today().strftime('%d-%m-%Y %H:%M:%S')\n",
    "                    df_attention = df_attention.fillna(\"\")\n",
    "\n",
    "                #sortby to get last trans_id value\n",
    "                df = df.sort_values(by=['ID_NPS'], ascending=True)\n",
    "\n",
    "                try:\n",
    "                    #check for identifier\n",
    "                    if globals()['util'][self.file][\"nfo\"][\"hasIdentifier\"] == \"y\":\n",
    "                        globals()[\"max_identifiervalue\"] = df[self.identifier].max()\n",
    "                    \n",
    "                    #insert into AzureDB the main df\n",
    "                    print (f'{self.functionName} starting mainInsertion time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "                    \n",
    "                    globals()['mainInsertTime'] = datetime.now()\n",
    "                    df = df.drop(columns=[self.identifier])\n",
    "                    print (globals()['util'][self.file][\"resultTempSuccessTable\"][self.functionName])\n",
    "                    \n",
    "                    df.to_sql(globals()['util'][self.file][\"resultTempSuccessTable\"][self.functionName], self.engine_azure, if_exists='replace', index=False)\n",
    "                    \n",
    "                    globals()['mainEndTime'] = datetime.now()\n",
    "                    globals()['rownumber'] = len(df)\n",
    "\n",
    "                    #for the main DataFrame - only call successHandle after the insertion is a success\n",
    "                    #successHandle(file= self.file, additionalInfo= \"\", runRowNumber= len(df), engine_azure = self.engine_azure)\n",
    "                    globals()['output'] += [{self.functionName : \"Success\"}]\n",
    "                except:\n",
    "                    globals()['output'] += [{self.functionName : \"failed_insertAzureDB\"}]\n",
    "                    #errorHandle(2, \"insertAzureDB\", None, self.file, self.engine_azure)\n",
    "            else:\n",
    "                #if there is no data after TransID then call successHandle\n",
    "                globals()['output'] += [{self.functionName : 'allClear'}]\n",
    "        except:\n",
    "            globals()['output'] += [{self.functionName : \"failed_DataFrame\"}]\n",
    "            #errorHandle(2, \"failedDataFrame\", None, self.file, self.engine_azure)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    #open auth file for azureDB\n",
    "    auth = open('auth.json')\n",
    "    auth_load = json.load(auth)\n",
    "    \n",
    "    #create AzureDB connection\n",
    "    engine_azure = getConnforMYSQL(auth_load, \"azureAccess\")\n",
    "    conn_azure = engine_azure.connect()\n",
    "\n",
    "    #create npsDB connection\n",
    "    engine_nps = getConnforMYSQL(auth_load, \"npsAccess\")\n",
    "    conn_nps = engine_nps.connect()\n",
    "\n",
    "    #get utilities content\n",
    "    util = open('utilities.json')\n",
    "    utilities_load = json.load(util)\n",
    "    globals()['util'] = utilities_load\n",
    "\n",
    "    list_dfAttention = []\n",
    "\n",
    "    #get identifier data\n",
    "    try:\n",
    "        if globals()['util'][file][\"nfo\"][\"hasIdentifier\"] == \"y\":\n",
    "            print (\"identifier detected\")\n",
    "            sql_text = sql_list[\"get_identifierData\"] %globals()['util'][file]['nfo']['runQueryName']\n",
    "            identifierData = executeSQL(conn_azure, sql_text)\n",
    "\n",
    "            #this should never return more than one line (because it gets num_linha = 1)\n",
    "            for row in identifierData.all():\n",
    "                identifier = row._mapping[\"identifier\"]\n",
    "                identifierValue = row._mapping[\"identifierValue\"]\n",
    "\n",
    "            if identifier == \"\" or identifierValue == \"\" or (identifier != globals()['util'][file][\"nfo\"][\"identifier\"] and identifier != globals()['util'][file][\"nfo\"][\"identifier\"].Uppercase()):\n",
    "                raise Exception\n",
    "        else:\n",
    "            print (\"identifier Not detected\")\n",
    "    except:\n",
    "        print ( \":)\" )\n",
    "\n",
    "    list_queries = globals()['util'][file]['queries']\n",
    "    threads = []\n",
    "    #setup output list for tracking\n",
    "    globals()['output'] = []\n",
    "    \n",
    "    for key, value in list_queries.items():\n",
    "        t = threadMain(key, value, identifier, identifierValue, file, engine_nps, engine_azure)\n",
    "        thread = t.getThread()\n",
    "        threads.append(thread)\n",
    "        thread.join()\n",
    "    \n",
    "    #after they finish, start merging (as upsert) temporary tables to the tables\n",
    "    output = None\n",
    "    for items in globals()['output']:\n",
    "        for key, value in items.items():\n",
    "            if value not in ['Success', 'allClear']:\n",
    "                output = 'failed'\n",
    "            else:\n",
    "                if value == 'Success' and output != 'failed':\n",
    "                    output = 'Success'\n",
    "                elif value == 'allClear' and output != 'failed':\n",
    "                    output = 'allClear'\n",
    "\n",
    "    #if all outputs are success then\n",
    "    if output == 'Success':\n",
    "        #do data merging with the temp tables\n",
    "        globals()['mainInsertTime'] = datetime.now()\n",
    "        #merge emails npsData\n",
    "        answer = executeSQL(conn_azure, sql_list['merge_emailsNpsData'])\n",
    "\n",
    "        #merge respostas npsData\n",
    "        answer = executeSQL(conn_azure, sql_list['merge_repostasNpsData'])\n",
    "        globals()['mainEndTime'] = datetime.now()\n",
    "\n",
    "        successHandle(file = file, additionalInfo= \"\", runRowNumber = globals()['rownumber'], engine_azure = engine_azure)\n",
    "        globals()['output'] = 'Success'\n",
    "\n",
    "\n",
    "    elif output == 'allClear':\n",
    "        #dont do data merging with the temp tables\n",
    "        #if there is no data after TransID then call successHandle\n",
    "        globals()[\"max_identifiervalue\"] = identifierValue\n",
    "        globals()['mainInsertTime'] = datetime.now()\n",
    "        globals()['mainEndTime'] = datetime.now()\n",
    "        successHandle(file = file, additionalInfo= \"no new id_nps\", runRowNumber= 0, engine_azure = engine_azure)\n",
    "        globals()['output'] = 'Success'\n",
    "\n",
    "    elif output == 'failed':\n",
    "        globals()['output'] = 'Failed'\n",
    "        \n",
    "    print (\"yey: \" , globals()['output'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRowNumber():\n",
    "    '''\n",
    "    to test the output of the function, first update example.json file in folder -> to do this, copy the output of the postman response in the dictionary\n",
    "    then run this function and compare the number of rows between AzureDB and the dataframe\n",
    "    '''\n",
    "    result = open(\"example.json\", 'r', encoding='utf-8')\n",
    "    result = json.loads(result.read())\n",
    "\n",
    "    df = pd.json_normalize(result['x'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nps_emailNps start time: 09/10/2023 09:28:38\n",
      "trying the dialect: ODBC Driver 18 for SQL Server\n",
      "engine created with dialect = ODBC Driver 18 for SQL Server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine test sucessful\n",
      "identifier detected\n",
      "starting nps_respostasNps @09/10/2023 09:28:46\n",
      "starting nps_emailNps @09/10/2023 09:28:50\n",
      "started successHandle\n",
      "yey:  Success\n",
      "nps_emailNps: done with the output: Success, runtime 15.191338\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file = \"nps_emailNps\"\n",
    "    print (f'{file} start time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "    globals()['startTime'] = datetime.now()\n",
    "    globals()['output'] = \"Failed\"\n",
    "    \n",
    "    main(file)\n",
    "\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    print('%s: done with the output: %s, runtime %s' %(file, globals()['output'], (globals()['endTime'] - globals()['startTime']).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

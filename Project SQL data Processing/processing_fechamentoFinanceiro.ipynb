{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, select, MetaData, Table\n",
    "import requests\n",
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "from datetime import date, datetime, timedelta\n",
    "from threading import Thread\n",
    "\n",
    "from sql_queries import sql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRowNumber():\n",
    "    '''\n",
    "    to test the output of the function, first update example.json file in folder -> to do this, copy the output of the postman response in the dictionary\n",
    "    then run this function and compare the number of rows between AzureDB and the dataframe\n",
    "    '''\n",
    "    result = open(\"example.json\", 'r', encoding='utf-8')\n",
    "    result = json.loads(result.read())\n",
    "\n",
    "    df = pd.json_normalize(result['x'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets connections for AzureDB\n",
    "def getConnforMYSQL(f_data, accessType):\n",
    "    list_dialects = pyodbc.drivers()\n",
    "    \n",
    "    for dialect in list_dialects:\n",
    "        try:\n",
    "            server = f_data[accessType][\"server\"]\n",
    "            db = f_data[accessType][\"database\"]\n",
    "            uid = f_data[accessType][\"uid\"]\n",
    "            pwd = f_data[accessType][\"pwd\"]\n",
    "            driver = f_data[accessType][\"dialect_driver\"]\n",
    "            port = f_data[accessType][\"port\"]\n",
    "\n",
    "            if accessType == \"azureAccess\":\n",
    "                if dialect in f_data[accessType][\"list_workingDialects\"]:\n",
    "                    print (f\"trying the dialect: {dialect}\")\n",
    "\n",
    "                    connection_string = (\n",
    "                        \" Driver={%s}\" %dialect +\n",
    "                        \"; SERVER=%s\" %server + \n",
    "                        \"; Database=%s \" %db + \n",
    "                        \"; UID=%s\" %uid +\n",
    "                        \"; PWD=%s\" %pwd\n",
    "                    )\n",
    "                    \n",
    "                    quoted = urllib.parse.quote_plus(connection_string)\n",
    "                    quoted = f_data[accessType][\"dialect_driver\"] + quoted\n",
    "                    #engine = create_engine(quoted, fast_executemany=True).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "                    engine = create_engine(quoted, fast_executemany=True)\n",
    "                    print (f\"engine created with dialect = {dialect}\")\n",
    "                    try:\n",
    "                        with engine.begin() as conn:\n",
    "                            df = pd.DataFrame([1], columns = ['test'])\n",
    "                            df.to_sql(\"connectionTestTable\", conn, if_exists=\"replace\", index = False)\n",
    "                            print(f\"engine test sucessful\")\n",
    "                            break\n",
    "                    except:\n",
    "                        print(f\"the dialect = {dialect} didn't work\")\n",
    "            else:\n",
    "                quoted = driver + uid + \":\" + pwd + \"@\" + server + \":\" + str(port) + \"/\" + db\n",
    "                engine = create_engine(quoted).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "            str_error = None\n",
    "\n",
    "        except:\n",
    "            print('exception found, trying other dialect')\n",
    "            pass\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get response from API\n",
    "def setupAPIrequest(utilities, extraParams):\n",
    "    '''\n",
    "    utilities: the utilies file\n",
    "    extraParams: extraParams as Dictionary for adding params in the request\n",
    "    '''\n",
    "    schemeHTTP = utilities[\"HTTP\"][\"schemeHTTP\"]\n",
    "    baseHTTP = utilities[\"HTTP\"][\"baseHTTP\"]\n",
    "    extraHTTP = utilities[\"HTTP\"][\"extraHTTP\"]\n",
    "    headers = utilities[\"HTTP\"][\"headers\"]\n",
    "    \n",
    "    #adds default headers\n",
    "    headers['Accept'] =  \"application/json\"\n",
    "    headers['Content-Type'] =  \"application/json\"   \n",
    "\n",
    "    #check if there is params variables:\n",
    "    paramsHTTP = \"\"\n",
    "    for key, value in utilities[\"HTTP\"].items():\n",
    "        if key == \"params\":\n",
    "            for key, value in utilities[\"HTTP\"][\"params\"].items():\n",
    "                paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "            paramsHTTP = \"?\" + paramsHTTP\n",
    "    if extraParams != \"\":\n",
    "        for key, value in extraParams.items():\n",
    "            paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "        paramsHTTP = paramsHTTP[:-1]\n",
    "    completeHTTP = schemeHTTP + baseHTTP + extraHTTP + paramsHTTP\n",
    "    \n",
    "    if utilities[\"HTTP\"][\"method\"] == \"get\":\n",
    "        response = requests.get(completeHTTP, headers=headers)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeSQL(conn_azure, sql_text):\n",
    "    '''\n",
    "    gets an AzureDB connection and a SQL code to run on the engine\n",
    "    Returns the result query\n",
    "    '''\n",
    "    query_answer = conn_azure.execute(sql_text)\n",
    "     \n",
    "    return query_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorHandle(errSeverity, errReason, additionalInfo, file, engine_azure):\n",
    "    '''\n",
    "    Handles error for logging in AzureDB:\n",
    "    errLocation should be: where is running, application that is running + file name, other info\n",
    "    errDescription should be: what went wrong probably\n",
    "    errProcedure should be: how to restart/check the schedule or other info + if it's ok to retry anytime\n",
    "    errSeverity: 1 to 5, where 1 is wait for next try and 5 is check immediately\n",
    "    the connection is the connection for the AzureDB\n",
    "    '''\n",
    "    print(\"started errorHandle\")\n",
    "\n",
    "    errProcedure = globals()['util'][\"errorSuggestedProcedure\"][errReason]\n",
    "    if additionalInfo != None:\n",
    "        errDescription = globals()['util'][\"errorDescription\"][errReason]\n",
    "    else:\n",
    "        errDescription = additionalInfo\n",
    "\n",
    "    errLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    errRunFileName = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    errRetry = globals()[\"util\"][file][\"nfo\"][\"retryOption\"]\n",
    "\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_errorLogTable (errorDescription, errorProcedure, errorStartTime, errorLocation, errorRetry, errorDuration, errorSeverity)\n",
    "        VALUES ('{errDescription}', '{errProcedure}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{errLocation}: {errRunFileName}', '{errRetry}', {timeDifference.total_seconds()}, {errSeverity}) \n",
    "    \"\"\"\n",
    "    #tabela = Table('nfo_errorLogTable', MetaData(), autoload_with=engine_azure)\n",
    "    #query = sa.insert(tabela).values(errorDescription = errDescription, errorProcedure = errProcedure, errorTime = datetime.now().strftime(\"%d/%m/%Y, %H:%M\"), errorLocation = errLocation, errorSeverity = errSeverity)\n",
    "    \n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the Success outputTable in AzureDB -> wms_vEstoqueConsultaSuccess\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started successHandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"nfo\"][\"runOutputSuccessTable\"]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "\n",
    "    #comes with insertion\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    \n",
    "    #should be changed to attention Len instead of time\n",
    "    globals()['attentionInsertTime'] = datetime.now()\n",
    "    globals()['attentionEndTime'] = datetime.now()\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    \n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    if globals()['util'][file][\"nfo\"][\"hasIdentifier\"] == \"y\":\n",
    "        sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo, identifier, identifierValue)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}', \n",
    "        '{globals()['util'][file][\"nfo\"][\"identifier\"]}' ,{globals()[\"max_identifiervalue\"]}) \n",
    "        \"\"\"\n",
    "\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the attention outputTable in AzureDB -> wms_vEstoqueConsultaAttention\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started attentionhandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"resultSuccessTable\"][file]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_attentionTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runInputLocation}', '{runQueryName}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber} , {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()},'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fCorrectTypes(dataFrame, columnsTypes_dict, list_dfAttention):\n",
    "    '''\n",
    "    gets a normalized data frame and a list of columns in a dictionary to change column type on the dataFrame\n",
    "    returns a list_dfAttention a list with datetime errors, dataframe with the altered columns \n",
    "    '''\n",
    "    for column in dataFrame:\n",
    "        for key, value in columnsTypes_dict.items():\n",
    "            if column == key:\n",
    "                data_type = value[\"type\"]\n",
    "                data_format = value[\"format\"]\n",
    "                #copy the df to errDataTime\n",
    "                errDataFrame = dataFrame\n",
    "\n",
    "                #remove empty column cells\n",
    "                errDataFrame = errDataFrame[errDataFrame[column].astype(bool)]\n",
    "                #reindex the errDateTime to match with mask\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                #create a mask where the convertion to datetime fails\n",
    "                if data_type == \"to_datetime\":\n",
    "                    mask = pd.to_datetime(errDataFrame[column], format=data_format, errors='coerce').isna()\n",
    "                if data_type == \"to_numeric\":\n",
    "                    mask = pd.to_numeric(errDataFrame[column], errors='coerce').isna()\n",
    "\n",
    "                #apply to df the mask from the substitution\n",
    "                errDataFrame = errDataFrame[mask]\n",
    "\n",
    "                #reindex the errDatetime\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                #append dataframe to be concatenated after only if there is > 1 row in the df\n",
    "                if len(errDataFrame) > 0:\n",
    "                    list_dfAttention.append(errDataFrame)\n",
    "\n",
    "                #the main Dataframe is kept with all the data (and the errors are coerced)\n",
    "                if data_type ==  \"to_datetime\":\n",
    "                    dataFrame[column].fillna(\"\", inplace=True)\n",
    "                    dataFrame[column] = pd.to_datetime(dataFrame[column], format=data_format, errors=\"coerce\")\n",
    "                if data_type == \"to_numeric\":\n",
    "                    dataFrame[column].fillna(0, inplace=True)\n",
    "                    #remove commas in case the numbers are stored as string\n",
    "                    dataFrame[column] = dataFrame[column].replace(regex = {'[^0-9]', ''})\n",
    "                    dataFrame[column] = dataFrame[column].replace(regex = {',', '.'})\n",
    "                    #change dType\n",
    "                    dataFrame[column] = pd.to_numeric(dataFrame[column], errors='coerce')\n",
    "                break\n",
    "        if dataFrame[column].dtype == int or dataFrame[column].dtype == float :\n",
    "            dataFrame[column].fillna(0, inplace=True)\n",
    "        else:\n",
    "            dataFrame[column].fillna(\"\", inplace=True)\n",
    "    return dataFrame, list_dfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class threadMain_datapipeline_fRelFechamento(object):\n",
    "    def __init__(self, numeroFilial, nomeFilial, startDate, endDate, file, engine_azure):\n",
    "        #set variables\n",
    "        self.numeroFilial = numeroFilial\n",
    "        self.nomeFilial = nomeFilial\n",
    "        self.startDate = startDate\n",
    "        self.endDate = endDate\n",
    "        self.engine_azure = engine_azure\n",
    "        self.file = file\n",
    "        #start the thread\n",
    "        self.t = Thread(target=self.threadMainTask, args=())\n",
    "        self.t.start()\n",
    "\n",
    "    def getThread(self):\n",
    "        return (self.t)\n",
    "    \n",
    "    def threadMainTask(self):\n",
    "        print(f'datapipeline_fRelFechamento starting filial: {self.nomeFilial} @{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "        #wait for good response from API endpoint\n",
    "        good_response = False\n",
    "        #repeat for 503 (server out of resources) and 504 (server request timeout) responses\n",
    "        while good_response == False:\n",
    "            #make dict with extraParams\n",
    "            extraParams = {\"filial\" : self.numeroFilial, \"data_inicial\" : self.startDate, \"data_final\" : self.endDate}\n",
    "\n",
    "            #request the response from the API\n",
    "            response = setupAPIrequest(globals()['util'][self.file], extraParams)\n",
    "            print('API response: %s' %response.status_code)\n",
    "\n",
    "            if response.status_code != 504 and response.status_code != 503:\n",
    "                good_response = True\n",
    "                        \n",
    "        if response.status_code == 200:\n",
    "            #add to the response pile\n",
    "            result_list = response.json()[\"value\"]\n",
    "            adjusted_list = [dict(item, numFilial = self.numeroFilial) for item in result_list]\n",
    "            globals()['response_list'] += adjusted_list\n",
    "            \n",
    "        else:\n",
    "            errorHandle(1, \"failedDataFrame\", \"External API response code: %s\" %response.status_code, self.file, self.engine_azure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_datapipeline_fRelFechamento(file, engine_azure, conn_azure, endDate, startDate):\n",
    "    #open auth file for azureDB\n",
    "    #auth = open('auth.json')\n",
    "    #auth_load = json.load(auth)\n",
    "    \n",
    "    #create AzureDB connection\n",
    "    #engine_azure = getConnforMYSQL(auth_load, \"azureAccess\")\n",
    "    #conn_azure = engine_azure.connect()\n",
    "\n",
    "    #get utilities content\n",
    "    #util = open('utilities.json')\n",
    "    #utilities_load = json.load(util)\n",
    "    #globals()['util'] = utilities_load\n",
    "\n",
    "    #first and last dates of the month\n",
    "    endDate = endDate.strftime(\"%Y-%m-%d\")\n",
    "    startDate = startDate.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    try:\n",
    "        #for millennium:\n",
    "        #get token\n",
    "        tokenResponse = setupAPIrequest(globals()['util'][\"mill_getToken\"], \"\")\n",
    "        token = json.loads(tokenResponse.text)[\"session\"]\n",
    "        \n",
    "        if tokenResponse.status_code != 504 and tokenResponse.status_code != 503:\n",
    "            print('token load response: %s' %tokenResponse.status_code)\n",
    "\n",
    "        #place token in the other header\n",
    "        globals()['util'][file][\"HTTP\"][\"headers\"][\"wts-session\"] = token\n",
    "    except:\n",
    "        print(\"token is broken\")\n",
    "        \n",
    "    #get list of filiais from AzureDB\n",
    "    globals()['response_list'] = []\n",
    "    threads = []\n",
    "    list_filiais = executeSQL(conn_azure, sql_list[\"get_filiaisList_datapipeline_fRelFechamento\"])\n",
    "    for row in list_filiais.all():\n",
    "        n_coluna = 0\n",
    "        for coluna in list_filiais.keys():\n",
    "            if coluna == \"numFilial\":\n",
    "                numeroFilial = row[n_coluna]\n",
    "            if coluna == \"nomeProcesso\":\n",
    "                nomeFilial = row[n_coluna]\n",
    "            n_coluna = n_coluna + 1\n",
    "        t = threadMain_datapipeline_fRelFechamento(numeroFilial, nomeFilial, startDate, endDate, file, engine_azure)\n",
    "        threads.append(t.getThread())\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    #setup global variable for the outcome of the connection\n",
    "    globals()['output'] = \"Failed\"    \n",
    "    \n",
    "    list_dfAttention = []      \n",
    "    \n",
    "    #the dataframe could be used instead of uploading to azure then querying in azure\n",
    "    try:\n",
    "        #start cleaning/changing dType of data\n",
    "        df = pd.json_normalize(globals()['response_list'])\n",
    "\n",
    "        df, list_dfAttention = fCorrectTypes(df, globals()['util'][file][\"columnsType_dict\"], list_dfAttention)\n",
    "\n",
    "        if len(globals()['response_list']) > 0:\n",
    "            try:\n",
    "                #insert into AzureDB the main df\n",
    "                print (f'{file} starting datapipeline_fRelFechamento mainInsertion time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "                df.to_sql(globals()['util'][file][\"resultSuccessTable\"][file], engine_azure, if_exists='replace', index=False)\n",
    "                \n",
    "                globals()['output'] = \"Success\"\n",
    "            except:\n",
    "                errorHandle(2, \"insertAzureDB_datapipeline_fRelFechamento\", None, file, engine_azure)\n",
    "        else:\n",
    "            globals()['output'] = 'Success'\n",
    "    except:\n",
    "        errorHandle(2, \"failedDataFrame_datapipeline_fRelFechamento\", None, file, engine_azure)\n",
    "    \n",
    "    print ('finished datapipeline_fRelFechamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class threadMain(object):\n",
    "    def __init__(self, numFilial, nomeProcesso, nomeFilialMill, codFilialMill, startDate, endDate, file, engine_azure):\n",
    "        #set variables\n",
    "        self.numFilial = numFilial\n",
    "        self.nomeFilialMill = nomeFilialMill\n",
    "        self.codFilialMill = codFilialMill\n",
    "        self.nomeProcesso = nomeProcesso\n",
    "\n",
    "        #sets variables for the rules\n",
    "        self.list_regrasGMV = []\n",
    "        self.list_regrasOther = []\n",
    "\n",
    "        #set dates\n",
    "        self.startDateDay = startDate.day\n",
    "        self.startDateMonth = startDate.month\n",
    "        self.startDateYear = startDate.year\n",
    "        self.endDateDay = endDate.day\n",
    "        self.endDateMonth = endDate.month\n",
    "        self.endDateYear = endDate.year\n",
    "        \n",
    "        #set engine and connection\n",
    "        self.engine_azure = engine_azure\n",
    "        self.conn_azure = self.engine_azure.connect()\n",
    "\n",
    "        self.file = file\n",
    "        #start the thread\n",
    "        self.t = Thread(target=self.threadMainTask, args=())\n",
    "        self.t.start()\n",
    "\n",
    "    def getThread(self):\n",
    "        return (self.t)\n",
    "    \n",
    "    def getAllRules(self):\n",
    "        #getGMV\n",
    "        list_getRegrasGMV = executeSQL(self.conn_azure, sql_list[\"getGMV\"] %self.nomeProcesso)\n",
    "        for row in list_getRegrasGMV:\n",
    "            nCol = 0\n",
    "            for column in list_getRegrasGMV.keys():\n",
    "                if column == 'mainidentifier':\n",
    "                    rule = row[nCol]\n",
    "                if column == 'subidentifier':\n",
    "                    subrule = row[nCol]\n",
    "                nCol += 1\n",
    "            self.list_regrasGMV += [{'rule' : rule ,'subrule' : subrule}]\n",
    "        self.list_regrasGMV = [dict(tupleized) for tupleized in set(tuple(item.items()) for item in self.list_regrasGMV)]\n",
    "\n",
    "        list_getRegrasOther = executeSQL(self.conn_azure, sql_list[\"getOthers\"] %self.nomeProcesso)\n",
    "        for row in list_getRegrasOther:\n",
    "            nCol = 0\n",
    "            for column in list_getRegrasOther.keys():\n",
    "                if column == 'mainrule':\n",
    "                    rule = row[nCol]\n",
    "                if column == 'subrule':\n",
    "                    subrule = row[nCol]\n",
    "                nCol += 1\n",
    "            self.list_regrasOther += [{'rule' : rule, 'subrule' : subrule}]\n",
    "        self.list_regrasOther = [dict(tupleized) for tupleized in set(tuple(item.items()) for item in self.list_regrasOther)]\n",
    "\n",
    "        \n",
    "    def threadMainTask(self):\n",
    "        if self.nomeProcesso != None and self.nomeProcesso != \"\":\n",
    "            print(f'starting filial: {self.numFilial} @{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "            self.getAllRules()\n",
    "        \n",
    "        #do GMV rules\n",
    "        for mainRule in self.list_regrasGMV:\n",
    "            rule = mainRule['rule']\n",
    "            subrule = mainRule['subrule']\n",
    "            found = False\n",
    "            match rule:\n",
    "                case \"SR\":\n",
    "                    match subrule:\n",
    "                        case \"MP\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"GMV_SR_MP\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"TM\":\n",
    "                    match subrule:\n",
    "                        case \"MP\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"GMV_TM_MP\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"SP\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"GMV_TM_SP\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"MF\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"GMV_TM_MF\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"VNF\":\n",
    "                    match subrule:\n",
    "                        case \"SP\":  \n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"GMV_VNF_SP\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "            if found:\n",
    "                for row in list_answer:\n",
    "                    globals()['response_list'] += [row]\n",
    "        #do otherRules\n",
    "        for otherRule in self.list_regrasOther:\n",
    "            rule = otherRule['rule']\n",
    "            subrule = otherRule['subrule']\n",
    "            found = False\n",
    "            match rule:\n",
    "                case \"ARMZ\":\n",
    "                    match subrule:\n",
    "                        case \"LC_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"ARMZ_LCPDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"SG_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"ARMZ_SGPDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"MOV\":\n",
    "                    match subrule:\n",
    "                        case \"MOV_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"MOV_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"MOV_SC\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"MOV_SC\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"FRETE\":\n",
    "                    match subrule:\n",
    "                        case \"FRT_CTE\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"FRT_CTE\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"FRT_COR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"FRT_COR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"VTEX\":\n",
    "                    match subrule:\n",
    "                        case \"VTX_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"VTX_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"AA_FIXO\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"AA_FIXO\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"WTL_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"WTL_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"SAC\":\n",
    "                    match subrule:\n",
    "                        case \"SAC_FX\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"SAC_FX\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"SAC_TVGMV\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"SAC_TVGMV\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                        case \"SAC_HRX\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"SAC_HRX\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"ESTORNO\":\n",
    "                    match subrule:\n",
    "                        case \"EST_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"EST_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"CADPROD\":\n",
    "                    match subrule:\n",
    "                        case \"CAD_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"CAD_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"SAB\":\n",
    "                    match subrule:\n",
    "                        case \"SAB_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"SAB_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"INV\":\n",
    "                    match subrule:\n",
    "                        case \"INV_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"INV_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "                case \"REEMBOLSO\":\n",
    "                    match subrule:\n",
    "                        case \"RMB_PDR\":\n",
    "                            list_answer = executeSQL(self.conn_azure, sql_list[\"RMB_PDR\"].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "                            found = True\n",
    "            if found:\n",
    "                for row in list_answer:\n",
    "                    globals()['response_list'] += [row]\n",
    "        #\n",
    "        #do relatorio analitico\n",
    "        list_analytic_reports = [\"GMV_REL\", \"ARMZ_REL\", \"MOV_REL\", \"ESTDIA_REL\", \"FRTCTE_REL\", \"FRTCOR_REL\"]\n",
    "        for item in list_analytic_reports:\n",
    "            list_answer = executeSQL(self.conn_azure, sql_list[item].format(startyear = self.startDateYear, startmonth = self.startDateMonth, startday = self.startDateDay, endyear = self.endDateYear, endmonth = self.endDateMonth, endday = self.endDateDay,  numerofilial = self.numFilial))\n",
    "            \n",
    "            for row in list_answer:\n",
    "                globals()['response_list_analytic_report'] += [row]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlcol(dfparam):    \n",
    "    \n",
    "    dtypedict = {}\n",
    "    for i,j in zip(dfparam.columns, dfparam.dtypes):\n",
    "        if \"object\" in str(j):\n",
    "            dtypedict.update({i: sa.types.VARCHAR(length=5000)})\n",
    "                                 \n",
    "        if \"datetime\" in str(j):\n",
    "            dtypedict.update({i: sa.types.DateTime()})\n",
    "\n",
    "        if \"float\" in str(j):\n",
    "            dtypedict.update({i: sa.types.Float(precision=3, asdecimal=True)})\n",
    "\n",
    "        if \"int\" in str(j):\n",
    "            dtypedict.update({i: sa.types.INT()})\n",
    "\n",
    "    return dtypedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    #open auth file for azureDB\n",
    "    auth = open('auth.json')\n",
    "    auth_load = json.load(auth)\n",
    "    \n",
    "    #create AzureDB connection\n",
    "    engine_azure = getConnforMYSQL(auth_load, \"azureAccess\")\n",
    "    conn_azure = engine_azure.connect()\n",
    "\n",
    "    #get utilities content\n",
    "    util = open('utilities.json')\n",
    "    utilities_load = json.load(util)\n",
    "    globals()['util'] = utilities_load\n",
    "\n",
    "    #first and last dates of the month\n",
    "    this_date = datetime(2023, 11, 1)\n",
    "    that_date = datetime(2023, 12, 1)\n",
    "    #endDate = (datetime.today().replace(day=1) - timedelta(days=1))\n",
    "    #startDate = ((datetime.today().replace(day=1) - timedelta(days=1)).replace(day=1))\n",
    "\n",
    "    #endDate = (this_date.replace(day=1) - timedelta(days=1))\n",
    "    #startDate = ((this_date.replace(day=1) - timedelta(days=1)).replace(day=1))\n",
    "\n",
    "    endDate = that_date\n",
    "    startDate = this_date\n",
    "\n",
    "    print ('enddate : ', endDate)\n",
    "    print ('startDate : ', startDate)\n",
    "\n",
    "    #makes the pipeline call\n",
    "    main_datapipeline_fRelFechamento('mill_fRelFechamento', engine_azure, conn_azure, endDate, startDate)\n",
    "\n",
    "    #get list of filiais of fechamento from AzureDB \n",
    "    globals()['response_list'] = []\n",
    "    globals()['response_list_analytic_report'] = []\n",
    "    threads = []\n",
    "    list_filiais = executeSQL(conn_azure, sql_list[\"getFiliaisList\"])\n",
    "    for row in list_filiais.all():\n",
    "        n_coluna = 0\n",
    "        for coluna in list_filiais.keys():\n",
    "            if coluna == \"numFilial\":\n",
    "                numFilial = row[n_coluna]\n",
    "            if coluna == \"nomeFilialMill\":\n",
    "                nomeFilialMill = row[n_coluna]\n",
    "            if coluna == \"codFilialMill\":\n",
    "                codFilialMill = row[n_coluna]\n",
    "            if coluna == \"nomeProcesso\":\n",
    "                nomeProcesso = row[n_coluna]\n",
    "            n_coluna = n_coluna + 1\n",
    "        t = threadMain(numFilial, nomeProcesso, nomeFilialMill, codFilialMill, startDate, endDate, file, engine_azure)\n",
    "        threads.append(t.getThread())\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    #setup global variable for the outcome of the connection\n",
    "    globals()['output'] = \"Failed\"\n",
    "    \n",
    "    #insert to azureDB\n",
    "    list_dfAttention = []      \n",
    "    try:\n",
    "        #start cleaning/changing dType of data\n",
    "        df_main = pd.DataFrame(globals()['response_list'])\n",
    "        df_analytic_report = pd.DataFrame(globals()['response_list_analytic_report'])\n",
    "        \n",
    "        df_main, list_dfAttention = fCorrectTypes(df_main, globals()['util'][file][\"columnsType_dict\"][\"processing_FechamentoFinanceiro\"], list_dfAttention)\n",
    "        df_analytic_report, list_dfAttention = fCorrectTypes(df_analytic_report, globals()['util'][file][\"columnsType_dict\"][\"processing_FechamentoAnalyticReport\"], list_dfAttention)\n",
    "        \n",
    "        df_main_dtypes = sqlcol(df_main)\n",
    "        df_analytic_report_dtypes = sqlcol(df_analytic_report)\n",
    "\n",
    "        if len(globals()['response_list']) > 0:\n",
    "            try:\n",
    "                #insert into AzureDB the main df\n",
    "                print (f'{file} starting mainInsertion time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "                globals()['mainInsertTime'] = datetime.now()\n",
    "\n",
    "                #main stuff\n",
    "                df_main.to_sql(utilities_load[file][\"resultSuccessTable\"][\"processing_FechamentoFinanceiro\"], engine_azure, if_exists='replace', index=False, dtype = df_main_dtypes)\n",
    "                globals()['mainEndTime'] = datetime.now()\n",
    "\n",
    "                #analytic report stuff\n",
    "                df_analytic_report.to_sql(utilities_load[file][\"resultSuccessTable\"][\"processing_FechamentoAnalyticReport\"], engine_azure, if_exists='replace', index=False, dtype = df_analytic_report_dtypes)\n",
    "                \n",
    "                #mark clocks\n",
    "                globals()['endTime'] = datetime.now()\n",
    "                globals()['attentionInsertTime'] = datetime.now()\n",
    "                globals()['attentionEndTime'] = datetime.now()\n",
    "\n",
    "                #for the main DataFrame\n",
    "                #successHandle(file= file, additionalInfo= \"\", runRowNumber= len(df), engine_azure= engine_azure)\n",
    "                globals()['output'] = \"Success\"\n",
    "            except:\n",
    "                errorHandle(2, \"insertAzureDB\", None, file, engine_azure)\n",
    "        else:\n",
    "            #if there is no data after TransID then call successHandle\n",
    "            globals()[\"max_identifiervalue\"] = None\n",
    "            globals()['mainInsertTime'] = datetime.now()\n",
    "            globals()['mainEndTime'] = datetime.now()\n",
    "            globals()['output'] = 'Success'\n",
    "            #successHandle(file= file, additionalInfo= \"no new trans_id\", runRowNumber= len(df), engine_azure = engine_azure)\n",
    "    except:\n",
    "        print('nayyyyyy')\n",
    "        #errorHandle(2, \"failedDataFrame\", None, file, engine_azure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing_FechamentoFinanceiro start time: 06/12/2023 20:25:20\n",
      "trying the dialect: ODBC Driver 18 for SQL Server\n",
      "engine created with dialect = ODBC Driver 18 for SQL Server\n",
      "engine test sucessful\n",
      "enddate :  2023-12-01 00:00:00\n",
      "startDate :  2023-11-01 00:00:00\n",
      "token load response: 200\n",
      "datapipeline_fRelFechamento starting filial: Abbott @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Amatime @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Beautyglam @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Biossance @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Clark @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: ClarkFull @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Coopermota @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Danone @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Docile @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Ecadeiras @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Essity @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Flowermind Co @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Roots2Go @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Roots2GoB2C @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Hth @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Industria Agro Cassava @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Issviva @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Jbl @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Jomer @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Karen @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Koffeeklub @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Lacta @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Levis @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Life @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Liffe @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Marelli @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Mboom @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Natural One @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Neeche @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Ontex @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Ontex Bigfral @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Oxitec @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Roots2go @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Schneider @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Sestini @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Sobrebarba @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Soho @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Steck @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Tpv @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Trox @06/12/2023 20:25:26\n",
      "datapipeline_fRelFechamento starting filial: Uniagro @06/12/2023 20:25:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucio.lee\\AppData\\Local\\Temp\\ipykernel_9652\\1859980836.py:6: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  query_answer = conn_azure.execute(sql_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "API response: 200\n",
      "mill_fRelFechamento starting datapipeline_fRelFechamento mainInsertion time: 06/12/2023 20:25:31\n",
      "finished datapipeline_fRelFechamento\n",
      "starting filial: 16 @06/12/2023 20:26:06\n",
      "starting filial: 19 @06/12/2023 20:26:06\n",
      "starting filial: 153 @06/12/2023 20:26:07\n",
      "starting filial: 153 @06/12/2023 20:26:08\n",
      "starting filial: 176 @06/12/2023 20:26:09\n",
      "starting filial: 205 @06/12/2023 20:26:09\n",
      "starting filial: 206 @06/12/2023 20:26:10\n",
      "starting filial: 213 @06/12/2023 20:26:11\n",
      "starting filial: 215 @06/12/2023 20:26:11\n",
      "starting filial: 218 @06/12/2023 20:26:11\n",
      "starting filial: 219 @06/12/2023 20:26:12\n",
      "starting filial: 223 @06/12/2023 20:26:12\n",
      "starting filial: 232 @06/12/2023 20:26:13\n",
      "starting filial: 239 @06/12/2023 20:26:14\n",
      "starting filial: 240 @06/12/2023 20:26:15\n",
      "starting filial: 20356 @06/12/2023 20:26:15\n",
      "starting filial: 20440 @06/12/2023 20:26:16\n",
      "starting filial: 20453 @06/12/2023 20:26:16\n",
      "starting filial: 20496 @06/12/2023 20:26:17\n",
      "starting filial: 20518 @06/12/2023 20:26:19\n",
      "starting filial: 20708 @06/12/2023 20:26:20\n",
      "starting filial: 20801 @06/12/2023 20:26:23\n",
      "starting filial: 20812 @06/12/2023 20:26:23\n",
      "starting filial: 20901 @06/12/2023 20:26:24\n",
      "starting filial: 21006 @06/12/2023 20:26:25\n",
      "starting filial: 21011 @06/12/2023 20:26:26\n",
      "starting filial: 21023 @06/12/2023 20:26:27\n",
      "starting filial: 21035 @06/12/2023 20:26:28\n",
      "starting filial: 21042 @06/12/2023 20:26:30\n",
      "starting filial: 21052 @06/12/2023 20:26:33\n",
      "starting filial: 21058 @06/12/2023 20:26:34\n",
      "starting filial: 21062 @06/12/2023 20:26:34\n",
      "starting filial: 21067 @06/12/2023 20:26:34\n",
      "starting filial: 21073 @06/12/2023 20:26:35\n",
      "starting filial: 21079 @06/12/2023 20:26:35\n",
      "starting filial: 21102 @06/12/2023 20:26:36\n",
      "starting filial: 21201 @06/12/2023 20:26:36\n",
      "starting filial: 21206 @06/12/2023 20:26:40\n",
      "starting filial: 21219 @06/12/2023 20:26:41\n",
      "starting filial: 21241 @06/12/2023 20:26:42\n",
      "starting filial: 41204 @06/12/2023 20:26:43\n",
      "processing_FechamentoFinanceiro starting mainInsertion time: 06/12/2023 20:27:07\n",
      "started errorHandle\n",
      "nayyyyyy\n",
      "processing_FechamentoFinanceiro: done with the output: Failed, runtime 120.361579\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file = \"processing_FechamentoFinanceiro\"\n",
    "    print (f'{file} start time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "    globals()['startTime'] = datetime.now()\n",
    "    \n",
    "    main(file)\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    print('%s: done with the output: %s, runtime %s' %(file, globals()['output'], (globals()['endTime'] - globals()['startTime']).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, select, MetaData, Table\n",
    "import requests\n",
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "from datetime import date, datetime, timedelta\n",
    "from threading import Thread\n",
    "\n",
    "from sql_queries import sql_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConnforMYSQL(f_data, accessType):\n",
    "    list_dialects = pyodbc.drivers()\n",
    "\n",
    "    for dialect in list_dialects:\n",
    "        try:\n",
    "            server = f_data[accessType][\"server\"]\n",
    "            db = f_data[accessType][\"database\"]\n",
    "            uid = f_data[accessType][\"uid\"]\n",
    "            pwd = f_data[accessType][\"pwd\"]\n",
    "            driver = f_data[accessType][\"dialect_driver\"]\n",
    "            port = f_data[accessType][\"port\"]\n",
    "\n",
    "            if accessType == \"azureAccess\":\n",
    "                if dialect in f_data[accessType][\"list_workingDialects\"]:\n",
    "                    print(f\"trying the dialect: {dialect}\")\n",
    "\n",
    "                    connection_string = (\n",
    "                        \" Driver={%s}\" % dialect\n",
    "                        + \"; SERVER=%s\" % server\n",
    "                        + \"; Database=%s \" % db\n",
    "                        + \"; UID=%s\" % uid\n",
    "                        + \"; PWD=%s\" % pwd\n",
    "                    )\n",
    "\n",
    "                    quoted = urllib.parse.quote_plus(connection_string)\n",
    "                    quoted = f_data[accessType][\"dialect_driver\"] + quoted\n",
    "                    # engine = create_engine(quoted, fast_executemany=True).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "                    engine = create_engine(quoted, fast_executemany=True)\n",
    "                    print(f\"engine created with dialect = {dialect}\")\n",
    "                    try:\n",
    "                        with engine.begin() as conn:\n",
    "                            df = pd.DataFrame([1], columns=[\"test\"])\n",
    "                            df.to_sql(\n",
    "                                \"connectionTestTable\",\n",
    "                                conn,\n",
    "                                if_exists=\"replace\",\n",
    "                                index=False,\n",
    "                            )\n",
    "                            print(f\"engine test sucessful\")\n",
    "                            break\n",
    "                    except:\n",
    "                        print(f\"the dialect = {dialect} didn't work\")\n",
    "            if accessType == \"millenniumAccess\":\n",
    "                if dialect in f_data[accessType][\"list_workingDialects\"]:\n",
    "                    print(f\"trying the dialect: {dialect}\")\n",
    "\n",
    "                    connection_string = (\n",
    "                        \" Driver={%s}\" % dialect\n",
    "                        + \"; SERVER=%s\" % server\n",
    "                        + \"; Database=%s \" % db\n",
    "                        + \"; UID=%s\" % uid\n",
    "                        + \"; PWD=%s\" % pwd\n",
    "                        + \"; Encrypt=no\"\n",
    "                        + \"; Mars_Connection=yes\"\n",
    "                    )\n",
    "\n",
    "                    quoted = urllib.parse.quote_plus(connection_string)\n",
    "                    quoted = f_data[accessType][\"dialect_driver\"] + quoted\n",
    "                    # engine = create_engine(quoted, fast_executemany=True).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "                    engine = create_engine(quoted, fast_executemany=True)\n",
    "                    print(f\"engine created with dialect = {dialect}\")\n",
    "            else:\n",
    "                print(driver + uid + \":\" + pwd + \"@\" + server + \":\" + str(port) + \"/\" + db)\n",
    "                quoted = (\n",
    "                    driver + uid + \":\" + pwd + \"@\" + server + \":\" + str(port) + \"/\" + db\n",
    "                )\n",
    "                engine = create_engine(quoted).execution_options(\n",
    "                    isolation_level=\"AUTOCOMMIT\"\n",
    "                )\n",
    "            str_error = None\n",
    "\n",
    "        except:\n",
    "            print(\"exception found, trying other dialect\")\n",
    "            pass\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupAPIrequest(utilities, extraParams):\n",
    "    \"\"\"\n",
    "    utilities: the utilies file\n",
    "    extraParams: extraParams as Dictionary for adding params in the request\n",
    "    \"\"\"\n",
    "    schemeHTTP = utilities[\"HTTP\"][\"schemeHTTP\"]\n",
    "    baseHTTP = utilities[\"HTTP\"][\"baseHTTP\"]\n",
    "    extraHTTP = utilities[\"HTTP\"][\"extraHTTP\"]\n",
    "    headers = utilities[\"HTTP\"][\"headers\"]\n",
    "\n",
    "    # adds default headers\n",
    "    headers[\"Accept\"] = \"application/json\"\n",
    "    headers[\"Content-Type\"] = \"application/json\"\n",
    "\n",
    "    # check if there is params variables:\n",
    "    paramsHTTP = \"\"\n",
    "    for key, value in utilities[\"HTTP\"].items():\n",
    "        if key == \"params\":\n",
    "            for key, value in utilities[\"HTTP\"][\"params\"].items():\n",
    "                paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "            paramsHTTP = \"?\" + paramsHTTP\n",
    "    if extraParams != \"\":\n",
    "        for key, value in extraParams.items():\n",
    "            paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "        paramsHTTP = paramsHTTP[:-1]\n",
    "    completeHTTP = schemeHTTP + baseHTTP + extraHTTP + paramsHTTP\n",
    "\n",
    "    if utilities[\"HTTP\"][\"method\"] == \"get\":\n",
    "        response = requests.get(completeHTTP, headers=headers)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeSQL(engine, sql_text):\n",
    "    \"\"\"\n",
    "    gets an connection and a SQL code to run on the engine\n",
    "    Returns the result query\n",
    "    \"\"\"\n",
    "    conn = engine.connect()\n",
    "    query_answer = conn.execute(sql_text)\n",
    "    keys = query_answer.keys()\n",
    "\n",
    "    answer = []\n",
    "    for row in query_answer:\n",
    "        n_coluna = 0\n",
    "        mid_answer = {}\n",
    "        for key in keys:\n",
    "            mid_answer[key] = row[n_coluna]\n",
    "            n_coluna += 1\n",
    "        answer += [mid_answer]\n",
    "    conn.close()\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorHandle(errSeverity, errReason, additionalInfo, file, engine_azure):\n",
    "    \"\"\"\n",
    "    Handles error for logging in AzureDB:\n",
    "    errLocation should be: where is running, application that is running + file name, other info\n",
    "    errDescription should be: what went wrong probably\n",
    "    errProcedure should be: how to restart/check the schedule or other info + if it's ok to retry anytime\n",
    "    errSeverity: 1 to 5, where 1 is wait for next try and 5 is check immediately\n",
    "    the connection is the connection for the AzureDB\n",
    "    \"\"\"\n",
    "    print(\"started errorHandle\")\n",
    "\n",
    "    errProcedure = globals()[\"util\"][\"errorSuggestedProcedure\"][errReason]\n",
    "    if additionalInfo != None:\n",
    "        errDescription = globals()[\"util\"][\"errorDescription\"][errReason]\n",
    "    else:\n",
    "        errDescription = additionalInfo\n",
    "\n",
    "    errLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    errRunFileName = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    errRetry = globals()[\"util\"][file][\"nfo\"][\"retryOption\"]\n",
    "\n",
    "    globals()[\"endTime\"] = datetime.now()\n",
    "    timeDifference = globals()[\"endTime\"] - globals()[\"startTime\"]\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_errorLogTable (errorDescription, errorProcedure, errorStartTime, errorLocation, errorRetry, errorDuration, errorSeverity)\n",
    "        VALUES ('{errDescription}', '{errProcedure}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{errLocation}: {errRunFileName}', '{errRetry}', {timeDifference.total_seconds()}, {errSeverity}) \n",
    "    \"\"\"\n",
    "    # tabela = Table('nfo_errorLogTable', MetaData(), autoload_with=engine_azure)\n",
    "    # query = sa.insert(tabela).values(errorDescription = errDescription, errorProcedure = errProcedure, errorTime = datetime.now().strftime(\"%d/%m/%Y, %H:%M\"), errorLocation = errLocation, errorSeverity = errSeverity)\n",
    "\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    \"\"\"\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime\n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the Success outputTable in AzureDB -> wms_vEstoqueConsultaSuccess\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    \"\"\"\n",
    "    print(\"started successHandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"nfo\"][\"runOutputSuccessTable\"]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "\n",
    "    globals()[\"endTime\"] = datetime.now()\n",
    "    timeDifference = globals()[\"endTime\"] - globals()[\"startTime\"]\n",
    "\n",
    "    # comes with insertion\n",
    "    mainInsertionTimeDifference = globals()[\"mainEndTime\"] - globals()[\"mainInsertTime\"]\n",
    "\n",
    "    # should be changed to attention Len instead of time\n",
    "    globals()[\"attentionInsertTime\"] = datetime.now()\n",
    "    globals()[\"attentionEndTime\"] = datetime.now()\n",
    "    attentionInsertionTimeDifference = (\n",
    "        globals()[\"attentionEndTime\"] - globals()[\"attentionInsertTime\"]\n",
    "    )\n",
    "\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    if globals()[\"util\"][file][\"nfo\"][\"hasIdentifier\"] == \"y\":\n",
    "        sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo, identifier, identifierValue)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}', \n",
    "        '{globals()['util'][file][\"nfo\"][\"identifier\"]}' ,{globals()[\"max_identifiervalue\"]}) \n",
    "        \"\"\"\n",
    "\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    \"\"\"\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime\n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the attention outputTable in AzureDB -> wms_vEstoqueConsultaAttention\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    \"\"\"\n",
    "    print(\"started attentionhandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"resultSuccessTable\"][file]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    timeDifference = globals()[\"endTime\"] - globals()[\"startTime\"]\n",
    "    mainInsertionTimeDifference = globals()[\"mainEndTime\"] - globals()[\"mainInsertTime\"]\n",
    "    attentionInsertionTimeDifference = (\n",
    "        globals()[\"attentionEndTime\"] - globals()[\"attentionInsertTime\"]\n",
    "    )\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_attentionTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runInputLocation}', '{runQueryName}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber} , {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()},'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fCorrectTypes(dataFrame, columnsTypes_dict, list_dfAttention):\n",
    "    \"\"\"\n",
    "    gets a normalized data frame and a list of columns in a dictionary to change column type on the dataFrame\n",
    "    returns a list_dfAttention a list with datetime errors, dataframe with the altered columns\n",
    "    \"\"\"\n",
    "    for column in dataFrame:\n",
    "        for key, value in columnsTypes_dict.items():\n",
    "            if column == key:\n",
    "                data_type = value[\"type\"]\n",
    "                data_format = value[\"format\"]\n",
    "                # copy the df to errDataTime\n",
    "                errDataFrame = dataFrame\n",
    "\n",
    "                # remove empty column cells\n",
    "                errDataFrame = errDataFrame[errDataFrame[column].astype(bool)]\n",
    "                # reindex the errDateTime to match with mask\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                # create a mask where the convertion to datetime fails\n",
    "                if data_type == \"to_datetime\":\n",
    "                    mask = pd.to_datetime(\n",
    "                        errDataFrame[column], format=data_format, errors=\"coerce\"\n",
    "                    ).isna()\n",
    "                if data_type == \"to_numeric\":\n",
    "                    mask = pd.to_numeric(errDataFrame[column], errors=\"coerce\").isna()\n",
    "\n",
    "                # apply to df the mask from the substitution\n",
    "                errDataFrame = errDataFrame[mask]\n",
    "\n",
    "                # reindex the errDatetime\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                # append dataframe to be concatenated after only if there is > 1 row in the df\n",
    "                if len(errDataFrame) > 0:\n",
    "                    list_dfAttention.append(errDataFrame)\n",
    "\n",
    "                # the main Dataframe is kept with all the data (and the errors are coerced)\n",
    "                if data_type == \"to_datetime\":\n",
    "                    dataFrame[column].fillna(\"\", inplace=True)\n",
    "                    dataFrame[column] = pd.to_datetime(\n",
    "                        dataFrame[column], format=data_format, errors=\"coerce\"\n",
    "                    )\n",
    "                if data_type == \"to_numeric\":\n",
    "                    dataFrame[column].fillna(0, inplace=True)\n",
    "                    # remove commas in case the numbers are stored as string\n",
    "                    dataFrame[column] = dataFrame[column].replace(regex={\"[^0-9]\", \"\"})\n",
    "                    dataFrame[column] = dataFrame[column].replace(regex={\",\", \".\"})\n",
    "                    # change dType\n",
    "                    dataFrame[column] = pd.to_numeric(\n",
    "                        dataFrame[column], errors=\"coerce\"\n",
    "                    )\n",
    "                break\n",
    "        if dataFrame[column].dtype == int or dataFrame[column].dtype == float:\n",
    "            dataFrame[column].fillna(0, inplace=True)\n",
    "        else:\n",
    "            dataFrame[column].fillna(\"\", inplace=True)\n",
    "    return dataFrame, list_dfAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_typecheck(dfparam):\n",
    "    '''\n",
    "    gets an pandas dataframe, checks its contents and returns a dictionary of the types of the columns\n",
    "    '''\n",
    "    dtypedict = {}\n",
    "    for i, j in zip(dfparam.columns, dfparam.dtypes):\n",
    "        if \"object\" in str(j):\n",
    "            dtypedict.update({i: sa.types.VARCHAR(length=255)})\n",
    "\n",
    "        if \"datetime\" in str(j):\n",
    "            dtypedict.update({i: sa.types.DateTime()})\n",
    "\n",
    "        if \"float\" in str(j):\n",
    "            dtypedict.update({i: sa.types.Float(precision=3, asdecimal=True)})\n",
    "\n",
    "        if \"int\" in str(j):\n",
    "            dtypedict.update({i: sa.types.INT()})\n",
    "\n",
    "    return dtypedict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class c_filial(object):\n",
    "    def __init__(self, numfilial):\n",
    "        self.numfilial = numfilial\n",
    "        self.list_barras = []\n",
    "        \n",
    "class c_barra(object):\n",
    "    def __init__(self,  codbarra=None):\n",
    "        self.codbarra = codbarra\n",
    "        self.list_nota = []\n",
    "\n",
    "class c_nota(object):\n",
    "    def __init__(self, data_movimento=None, data_nf=None, quantity=None, price=None, nota=None, custo_medio=None):\n",
    "        self.data_movimento = data_movimento\n",
    "        self.data_nf = data_nf\n",
    "        self.quantity = quantity\n",
    "        self.price = price\n",
    "        self.nota = nota\n",
    "        self.custo_medio = custo_medio\n",
    "\n",
    "        self.fufilled_quantity = None\n",
    "        self.status = 'e'\n",
    "        #p = partial\n",
    "        #f = fullied\n",
    "        #e = empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thread_gatherdata (object):\n",
    "    def __init__(self, file, engine):\n",
    "        self.engine = engine\n",
    "\n",
    "        #start the thread\n",
    "        self.t = Thread(target=self.Thread_gatherdataMainTask, args=())\n",
    "        self.t.start()\n",
    "\n",
    "    def getThread(self):\n",
    "        return (self.t)\n",
    "\n",
    "    def Thread_gatherdataMainTask(self):\n",
    "        list_produtos = executeSQL(self.engine, sql_list[\"get_entradas\"])\n",
    "        for each_row in list_produtos:\n",
    "            numfilial = each_row['filial']\n",
    "            data_movimento = each_row['data_movimento']\n",
    "            data_nf = each_row['data_nf']\n",
    "            nota = each_row['nota']\n",
    "            codbarra = each_row['codbarra']\n",
    "            codproduto = each_row['codproduto']\n",
    "            produto = each_row['produto']\n",
    "            codcor = each_row['codcor']\n",
    "            codtamanho = each_row['codtamanho']\n",
    "            quantidade = each_row['quantidade']\n",
    "            preco = each_row['preco']\n",
    "            custo_medio = each_row['custo_medio']\n",
    "\n",
    "        \n",
    "            if len(globals()[\"list_obj_filial\"]) == 0 :\n",
    "                dummyfilial = c_filial(numfilial)\n",
    "                dummybarra = c_barra(codbarra)\n",
    "                dummynota = c_nota(data_movimento, data_nf, quantidade, preco, nota, custo_medio)\n",
    "\n",
    "                dummybarra.list_nota  += [dummynota]\n",
    "                dummyfilial.list_barras  += [dummybarra]\n",
    "\n",
    "                globals()[\"list_obj_filial\"] += [dummyfilial]\n",
    "            else:\n",
    "                for filial in globals()[\"list_obj_filial\"]:\n",
    "                    if filial.numfilial == numfilial:\n",
    "                        for barra in filial.list_barras:\n",
    "                            if barra.codbarra == codbarra:\n",
    "                                for colour in product.list_codcolour:\n",
    "                                    if colour.codcolour == codcor:\n",
    "                                        for size in colour.list_codsize:\n",
    "                                            if size.codsize == codtamanho:\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                dummysize = c_codsize(data, codtamanho, quantidade, preco, nota, custo_medio)\n",
    "                                                colour.list_codsize += [dummysize]\n",
    "                                        break\n",
    "                                    else:\n",
    "                                        dummycolour = c_codcolour(codcor)\n",
    "                                        dummysize = c_codsize(data, codtamanho, quantidade, preco, nota, custo_medio)\n",
    "\n",
    "                                        dummycolour.list_codsize += [dummysize]\n",
    "                                        product.list_codcolour += [dummycolour]\n",
    "                                break\n",
    "                            else:\n",
    "                                #if product not in list, insert everything\n",
    "                                dummyproduct = c_product(codproduto)\n",
    "                                dummycolour = c_codcolour(codcor)\n",
    "                                dummysize = c_codsize(data, codtamanho, quantidade, preco, nota, custo_medio)\n",
    "\n",
    "                                dummycolour.list_codsize += [dummysize] \n",
    "                                dummyproduct.list_codcolour  += [dummycolour]\n",
    "                                filial.list_products += [dummyproduct] \n",
    "                        break\n",
    "                    else:\n",
    "                        dummyfilial = c_filial(numfilial)\n",
    "                        dummyproduct = c_product(codproduto)\n",
    "                        dummycolour = c_codcolour(codcor)\n",
    "                        dummysize = c_codsize(data, codtamanho, quantidade, preco, nota, custo_medio)\n",
    "\n",
    "                        dummycolour.list_codsize += [dummysize] \n",
    "                        dummyproduct.list_codcolour  += [dummycolour]\n",
    "                        dummyfilial.list_products += [dummyproduct]\n",
    "\n",
    "                        globals()[\"list_obj_filial\"] += [dummyfilial]\n",
    "\n",
    "                        \n",
    "                    print (len(filial.list_products))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    # open auth file for azureDB\n",
    "    auth = open(\"auth.json\")\n",
    "    auth_load = json.load(auth)\n",
    "\n",
    "    # create AzureDB connection\n",
    "    engine_azure = getConnforMYSQL(auth_load, \"azureAccess\")\n",
    "    conn_azure = engine_azure.connect()\n",
    "\n",
    "    #create millenniumDB connection\n",
    "    engine_mill = getConnforMYSQL(auth_load, \"millenniumAccess\")\n",
    "    conn_mill = engine_mill.connect()\n",
    "\n",
    "    # get utilities content\n",
    "    util = open(\"utilities.json\")\n",
    "    utilities_load = json.load(util)\n",
    "    globals()[\"util\"] = utilities_load\n",
    "\n",
    "    #get distinct filials from millennium, make a thread for each of them\n",
    "    globals()['list_filials'] = []\n",
    "    threads = []\n",
    "    list_dict_filiais = executeSQL(engine_mill, sql_list[\"getFiliaisList\"])\n",
    "\n",
    "    globals()[\"list_obj_filial\"] = []\n",
    "\n",
    "    t = Thread_gatherdata(file, engine_mill)\n",
    "    threads.append(t.getThread())\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapipeline_millennium_recebimentos start time: 10/12/2023 21:11:06\n",
      "mssql+pyodbc:///?odbc_connect=selia:Qwerty1234!@bi-selia.database.windows.net:1433/bi\n",
      "trying the dialect: ODBC Driver 18 for SQL Server\n",
      "engine created with dialect = ODBC Driver 18 for SQL Server\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine test sucessful\n",
      "trying the dialect: ODBC Driver 17 for SQL Server\n",
      "engine created with dialect = ODBC Driver 17 for SQL Server\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucio.lee\\AppData\\Local\\Temp\\ipykernel_3516\\305422065.py:7: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  query_answer = conn.execute(sql_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file = \"datapipeline_millennium_recebimentos\"\n",
    "    print(f'{file} start time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "    globals()[\"startTime\"] = datetime.now()\n",
    "\n",
    "    main(file)\n",
    "    globals()[\"endTime\"] = datetime.now()\n",
    "    print(\n",
    "        \"%s: done with the output: %s, runtime %s\"\n",
    "        % (\n",
    "            file,\n",
    "            globals()[\"output\"],\n",
    "            (globals()[\"endTime\"] - globals()[\"startTime\"]).total_seconds(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

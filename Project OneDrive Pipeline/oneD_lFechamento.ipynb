{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, select, MetaData, Table\n",
    "import requests\n",
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "from datetime import date, datetime, timedelta\n",
    "from threading import Thread\n",
    "\n",
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "from office365.sharepoint.folders.folder import Folder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets connections for AzureDB\n",
    "def getConnforMYSQL(f_data, accessType):\n",
    "    list_dialects = pyodbc.drivers()\n",
    "    \n",
    "    for dialect in list_dialects:\n",
    "        try:\n",
    "            server = f_data[accessType][\"server\"]\n",
    "            db = f_data[accessType][\"database\"]\n",
    "            uid = f_data[accessType][\"uid\"]\n",
    "            pwd = f_data[accessType][\"pwd\"]\n",
    "            driver = f_data[accessType][\"dialect_driver\"]\n",
    "            port = f_data[accessType][\"port\"]\n",
    "\n",
    "            if accessType == \"azureAccess\":\n",
    "                if dialect in f_data[accessType][\"list_workingDialects\"]:\n",
    "                    print (f\"trying the dialect: {dialect}\")\n",
    "\n",
    "                    connection_string = (\n",
    "                        \" Driver={%s}\" %dialect +\n",
    "                        \"; SERVER=%s\" %server + \n",
    "                        \"; Database=%s \" %db + \n",
    "                        \"; UID=%s\" %uid +\n",
    "                        \"; PWD=%s\" %pwd\n",
    "                    )\n",
    "                    \n",
    "                    quoted = urllib.parse.quote_plus(connection_string)\n",
    "                    quoted = f_data[accessType][\"dialect_driver\"] + quoted\n",
    "                    #engine = create_engine(quoted, fast_executemany=True).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "                    engine = create_engine(quoted, fast_executemany=True)\n",
    "                    print (f\"engine created with dialect = {dialect}\")\n",
    "                    try:\n",
    "                        with engine.begin() as conn:\n",
    "                            df = pd.DataFrame([1], columns = ['test'])\n",
    "                            df.to_sql(\"connectionTestTable\", conn, if_exists=\"replace\", index = False)\n",
    "                            print(f\"engine test sucessful\")\n",
    "                            break\n",
    "                    except:\n",
    "                        print(f\"the dialect = {dialect} didn't work\")\n",
    "            else:\n",
    "                quoted = driver + uid + \":\" + pwd + \"@\" + server + \":\" + str(port) + \"/\" + db\n",
    "                engine = create_engine(quoted).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "            str_error = None\n",
    "\n",
    "        except:\n",
    "            print('exception found, trying other dialect')\n",
    "            pass\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get response from API\n",
    "def setupAPIrequest(utilities, extraParams):\n",
    "    '''\n",
    "    utilities: the utilies file\n",
    "    extraParams: extraParams as Dictionary for adding params in the request\n",
    "    '''\n",
    "    schemeHTTP = utilities[\"HTTP\"][\"schemeHTTP\"]\n",
    "    baseHTTP = utilities[\"HTTP\"][\"baseHTTP\"]\n",
    "    extraHTTP = utilities[\"HTTP\"][\"extraHTTP\"]\n",
    "    headers = utilities[\"HTTP\"][\"headers\"]\n",
    "    \n",
    "    #adds default headers\n",
    "    headers['Accept'] =  \"application/json\"\n",
    "    headers['Content-Type'] =  \"application/json\"   \n",
    "\n",
    "    #check if there is params variables:\n",
    "    paramsHTTP = \"\"\n",
    "    for key, value in utilities[\"HTTP\"].items():\n",
    "        if key == \"params\":\n",
    "            for key, value in utilities[\"HTTP\"][\"params\"].items():\n",
    "                paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "            paramsHTTP = \"?\" + paramsHTTP\n",
    "    if extraParams != \"\":\n",
    "        for key, value in extraParams.items():\n",
    "            paramsHTTP = paramsHTTP + key + \"=\" + str(value) + \"&\"\n",
    "        paramsHTTP = paramsHTTP[:-1]\n",
    "    completeHTTP = schemeHTTP + baseHTTP + extraHTTP + paramsHTTP\n",
    "    \n",
    "    if utilities[\"HTTP\"][\"method\"] == \"get\":\n",
    "        response = requests.get(completeHTTP, headers=headers)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to sharepoint\n",
    "def getConnForSharepoint (user, password, download_path, file_url, url):\n",
    "    ctx_auth = AuthenticationContext(url)\n",
    "    ctx_auth.acquire_token_for_user(user, password)   \n",
    "    ctx = ClientContext(url, ctx_auth)\n",
    "    \n",
    "    with open(download_path, \"wb\") as local_file:\n",
    "        file = ctx.web.get_file_by_server_relative_url(file_url).download(local_file).execute_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorHandle(errSeverity, errReason, additionalInfo, file, engine_azure):\n",
    "    '''\n",
    "    Handles error for logging in AzureDB:\n",
    "    errLocation should be: where is running, application that is running + file name, other info\n",
    "    errDescription should be: what went wrong probably\n",
    "    errProcedure should be: how to restart/check the schedule or other info + if it's ok to retry anytime\n",
    "    errSeverity: 1 to 5, where 1 is wait for next try and 5 is check immediately\n",
    "    the connection is the connection for the AzureDB\n",
    "    '''\n",
    "    print(\"started errorHandle\")\n",
    "\n",
    "    errProcedure = globals()['util'][\"errorSuggestedProcedure\"][errReason]\n",
    "    if additionalInfo != None:\n",
    "        errDescription = globals()['util'][\"errorDescription\"][errReason]\n",
    "    else:\n",
    "        errDescription = additionalInfo\n",
    "\n",
    "    errLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    errRunFileName = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    errRetry = globals()[\"util\"][file][\"nfo\"][\"retryOption\"]\n",
    "\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_errorLogTable (errorDescription, errorProcedure, errorStartTime, errorLocation, errorRetry, errorDuration, errorSeverity)\n",
    "        VALUES ('{errDescription}', '{errProcedure}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{errLocation}: {errRunFileName}', '{errRetry}', {timeDifference.total_seconds()}, {errSeverity}) \n",
    "    \"\"\"\n",
    "    #tabela = Table('nfo_errorLogTable', MetaData(), autoload_with=engine_azure)\n",
    "    #query = sa.insert(tabela).values(errorDescription = errDescription, errorProcedure = errProcedure, errorTime = datetime.now().strftime(\"%d/%m/%Y, %H:%M\"), errorLocation = errLocation, errorSeverity = errSeverity)\n",
    "    \n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the Success outputTable in AzureDB -> wms_vEstoqueConsultaSuccess\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started successHandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"nfo\"][\"runOutputSuccessTable\"]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "\n",
    "    globals()['endTime'] = datetime.now()\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "\n",
    "    #comes with insertion\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    \n",
    "    #should be changed to attention Len instead of time\n",
    "    globals()['attentionInsertTime'] = datetime.now()\n",
    "    globals()['attentionEndTime'] = datetime.now()\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    \n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    if globals()['util'][file][\"nfo\"][\"hasIdentifier\"] == \"y\":\n",
    "        sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo, identifier, identifierValue)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}', \n",
    "        '{globals()['util'][file][\"nfo\"][\"identifier\"]}' ,{globals()[\"max_identifiervalue\"]}) \n",
    "        \"\"\"\n",
    "\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the attention outputTable in AzureDB -> wms_vEstoqueConsultaAttention\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started attentionhandle\")\n",
    "    runFile = globals()[\"util\"][file][\"nfo\"][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"nfo\"][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"nfo\"][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"resultAttentionTable\"][file]\n",
    "    runLocation = globals()[\"util\"][file][\"nfo\"][\"runLocation\"]\n",
    "    \n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    \n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_attentionTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runInputLocation}', '{runQueryName}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber} , {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()},'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fCorrectTypes(dataFrame, columnsTypes_dict, list_dfAttention):\n",
    "    '''\n",
    "    gets a normalized data frame and a list of columns in a dictionary to change column type on the dataFrame\n",
    "    returns a list_dfAttention a list with datetime errors, dataframe with the altered columns \n",
    "    '''\n",
    "    for column in dataFrame:\n",
    "        for key, value in columnsTypes_dict.items():\n",
    "            if column == key:\n",
    "                data_type = value[\"type\"]\n",
    "                data_format = value[\"format\"]\n",
    "                #copy the df to errDataTime\n",
    "                errDataFrame = dataFrame\n",
    "\n",
    "                #remove empty column cells\n",
    "                errDataFrame = errDataFrame[errDataFrame[column].astype(bool)]\n",
    "                #reindex the errDateTime to match with mask\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                #create a mask where the convertion to datetime fails\n",
    "                if data_type == \"to_datetime\":\n",
    "                    mask = pd.to_datetime(errDataFrame[column], format=data_format, errors='coerce').isna()\n",
    "                if data_type == \"to_numeric\":\n",
    "                    mask = pd.to_numeric(errDataFrame[column], errors='coerce').isna()\n",
    "\n",
    "                #apply to df the mask from the substitution\n",
    "                errDataFrame = errDataFrame[mask]\n",
    "\n",
    "                #reindex the errDatetime\n",
    "                errDataFrame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                #append dataframe to be concatenated after only if there is > 1 row in the df\n",
    "                if len(errDataFrame) > 0:\n",
    "                    list_dfAttention.append(errDataFrame)\n",
    "\n",
    "                #the main Dataframe is kept with all the data (and the errors are coerced)\n",
    "                if data_type ==  \"to_datetime\":\n",
    "                    dataFrame[column].fillna(\"\", inplace=True)\n",
    "                    dataFrame[column] = pd.to_datetime(dataFrame[column], format=data_format, errors=\"coerce\")\n",
    "                    dataFrame[column] = dataFrame[column].dt.tz_localize(None)\n",
    "                if data_type == \"to_numeric\":\n",
    "                    dataFrame[column].fillna(0, inplace=True)\n",
    "                    #remove commas in case the numbers are stored as string\n",
    "                    dataFrame[column] = dataFrame[column].replace(regex = {'[^0-9]', ''})\n",
    "                    dataFrame[column] = dataFrame[column].replace(regex = {',', '.'})\n",
    "                    #change dType\n",
    "                    dataFrame[column] = pd.to_numeric(dataFrame[column], errors='coerce')\n",
    "                break\n",
    "        if dataFrame[column].dtype == int or dataFrame[column].dtype == float :\n",
    "            dataFrame[column].fillna(0, inplace=True)\n",
    "        else:\n",
    "            dataFrame[column].fillna(\"\", inplace=True)\n",
    "    return dataFrame, list_dfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    #open auth file for azureDB\n",
    "    auth = open('auth.json')\n",
    "    auth_load = json.load(auth)\n",
    "    \n",
    "    #create AzureDB connection\n",
    "    engine_azure = getConnforMYSQL(auth_load, \"azureAccess\")\n",
    "    conn_azure = engine_azure.connect()\n",
    "\n",
    "    #get utilities content\n",
    "    util = open('utilities.json')\n",
    "    utilities_load = json.load(util)\n",
    "    globals()['util'] = utilities_load\n",
    "\n",
    "    list_dfAttention = []\n",
    "\n",
    "    #download OneD files to this folder\n",
    "    try:\n",
    "        getConnForSharepoint(auth_load[\"OneDAccess\"][\"user\"], auth_load[\"OneDAccess\"][\"password\"], utilities_load[file][\"HTTP\"][\"download_path\"], utilities_load[file][\"HTTP\"][\"file_url\"], utilities_load[file][\"HTTP\"][\"oneD_url\"])\n",
    "        print(f\"Download from oneD Successful. Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "        #if the the download is OK, then\n",
    "        try:\n",
    "            #creating a dataframe with the excel file\n",
    "            df_regrasGMV = pd.read_excel(\"./file.xlsx\", sheet_name='RegraGMV')\n",
    "            df_regrasOther = pd.read_excel(\"./file.xlsx\", sheet_name='RegrasOther')\n",
    "            df_filiais = pd.read_excel(\"./file.xlsx\", sheet_name='filiais')\n",
    "            df_nomenclaturaARMZ = pd.read_excel(\"./file.xlsx\", sheet_name='nomenclaturaARMZ')\n",
    "            df_valoresFormulario = pd.read_excel(\"./file.xlsx\", sheet_name='valoresFormulario')\n",
    "            df_regraMarkupTransportadoras = pd.read_excel(\"./file.xlsx\", sheet_name='regraMarkupTransportadoras')\n",
    "            df_faturaCorreios = pd.read_excel(\"./file.xlsx\", sheet_name='faturaCorreios')\n",
    "            df_regraMarkupFRTClientes = pd.read_excel(\"./file.xlsx\", sheet_name='regraMarkupFRTClientes')\n",
    "\n",
    "            #correct type\n",
    "            df_regrasGMV, list_dfAttention = fCorrectTypes(df_regrasGMV, globals()['util'][file][\"columnsType_dict\"][\"regrasGMV\"] ,list_dfAttention)\n",
    "            df_regrasOther, list_dfAttention = fCorrectTypes(df_regrasOther, globals()['util'][file][\"columnsType_dict\"][\"regrasOther\"] ,list_dfAttention)\n",
    "            df_filiais, list_dfAttention = fCorrectTypes(df_filiais, globals()['util'][file][\"columnsType_dict\"][\"filiais\"] ,list_dfAttention)\n",
    "            df_nomenclaturaARMZ, list_dfAttention = fCorrectTypes(df_nomenclaturaARMZ, globals()['util'][file][\"columnsType_dict\"][\"nomenclaturaARMZ\"] ,list_dfAttention)\n",
    "            df_valoresFormulario, list_dfAttention = fCorrectTypes(df_valoresFormulario, globals()['util'][file][\"columnsType_dict\"][\"valoresFormulario\"] ,list_dfAttention)\n",
    "            df_regraMarkupTransportadoras, list_dfAttention = fCorrectTypes(df_regraMarkupTransportadoras, globals()['util'][file][\"columnsType_dict\"][\"regraMarkupTransportadoras\"] ,list_dfAttention)\n",
    "            df_faturaCorreios, list_dfAttention = fCorrectTypes(df_faturaCorreios, globals()['util'][file][\"columnsType_dict\"][\"faturaCorreios\"] ,list_dfAttention)\n",
    "            df_regraMarkupFRTClientes, list_dfAttention = fCorrectTypes(df_regraMarkupFRTClientes, globals()['util'][file][\"columnsType_dict\"][\"regraMarkupFRTClientes\"] ,list_dfAttention)\n",
    "\n",
    "            try:\n",
    "                #insert into AzureDB the main df\n",
    "                print (f'{file} starting mainInsertion time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "\n",
    "                globals()['mainInsertTime'] = datetime.now()\n",
    "                df_regrasGMV.to_sql(utilities_load[file][\"resultSuccessTable\"][\"regrasGMV\"], engine_azure, if_exists='replace', index=False)\n",
    "                df_regrasOther.to_sql(utilities_load[file][\"resultSuccessTable\"][\"regrasOther\"], engine_azure, if_exists='replace', index=False)\n",
    "                df_filiais.to_sql(utilities_load[file][\"resultSuccessTable\"][\"filiais\"], engine_azure, if_exists='replace', index=False)\n",
    "                df_nomenclaturaARMZ.to_sql(utilities_load[file][\"resultSuccessTable\"][\"nomenclaturaARMZ\"], engine_azure, if_exists='replace', index=False)\n",
    "                df_valoresFormulario.to_sql(utilities_load[file][\"resultSuccessTable\"][\"valoresFormulario\"], engine_azure, if_exists='replace', index=False)\n",
    "                df_regraMarkupTransportadoras.to_sql(utilities_load[file][\"resultSuccessTable\"][\"regraMarkupTransportadoras\"], engine_azure, if_exists='replace', index=False)\n",
    "                df_faturaCorreios.to_sql(utilities_load[file][\"resultSuccessTable\"][\"faturaCorreios\"], engine_azure, if_exists='replace', index=False)\n",
    "                df_regraMarkupFRTClientes.to_sql(utilities_load[file][\"resultSuccessTable\"][\"regraMarkupFRTClientes\"], engine_azure, if_exists='replace', index=False)\n",
    "                globals()['mainEndTime'] = datetime.now()\n",
    "                \n",
    "                #mark clocks\n",
    "                globals()['endTime'] = datetime.now()\n",
    "                globals()['attentionInsertTime'] = datetime.now()\n",
    "                globals()['attentionEndTime'] = datetime.now()\n",
    "\n",
    "                #for the main DataFrame\n",
    "                successHandle(file=file, additionalInfo= \"\", runRowNumber= (len(df_regrasGMV) + len(df_regrasOther) + len(df_filiais) + len(df_nomenclaturaARMZ) + len(df_valoresFormulario)), engine_azure = engine_azure)\n",
    "                globals()['output'] = \"Success\"\n",
    "            except:\n",
    "                errorHandle(2, \"InsertAzureDB\", None, file, engine_azure)\n",
    "        except:\n",
    "            errorHandle(5, \"FailedDataFrame\", None, file, engine_azure)\n",
    "    except:\n",
    "        print(f\"failed to download from oneD. Time: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "        errorHandle(5, \"OneD\", None, \"oneD_utilities\", engine_azure)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRowNumber():\n",
    "    '''\n",
    "    to test the output of the function, first update example.json file in folder -> to do this, copy the output of the postman response in the dictionary\n",
    "    then run this function and compare the number of rows between AzureDB and the dataframe\n",
    "    '''\n",
    "    result = open(\"example.json\", 'r', encoding='utf-8')\n",
    "    result = json.loads(result.read())\n",
    "\n",
    "    df = pd.json_normalize(result['x'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneD_utilities start time: 28/11/2023 17:36:20\n",
      "trying the dialect: ODBC Driver 18 for SQL Server\n",
      "engine created with dialect = ODBC Driver 18 for SQL Server\n",
      "engine test sucessful\n",
      "Download from oneD Successful. Time: 28/11/2023 17:36:27\n",
      "oneD_utilities starting mainInsertion time: 28/11/2023 17:36:29\n",
      "started successHandle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucio.lee\\AppData\\Local\\Temp\\ipykernel_17816\\3086440147.py:45: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  conn.execute(sql_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneD_utilities: done with the output: Success, runtime 48.496022\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file = \"oneD_utilities\"\n",
    "    print (f'{file} start time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "    globals()['startTime'] = datetime.now()\n",
    "    globals()['output'] = \"Failed\"\n",
    "    \n",
    "    main(file)\n",
    "\n",
    "    print('%s: done with the output: %s, runtime %s' %(file, globals()['output'], (globals()['endTime'] - globals()['startTime']).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

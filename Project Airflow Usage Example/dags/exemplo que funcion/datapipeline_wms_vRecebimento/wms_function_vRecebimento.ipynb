{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, select, MetaData, Table\n",
    "import requests\n",
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "from datetime import date, datetime, timedelta\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets connections for AzureDB\n",
    "def getConnforMYSQL(f_data, accessType):\n",
    "    list_dialects = pyodbc.drivers()\n",
    "    \n",
    "    for dialect in list_dialects:\n",
    "        try:\n",
    "            server = f_data[accessType][\"server\"]\n",
    "            db = f_data[accessType][\"database\"]\n",
    "            uid = f_data[accessType][\"uid\"]\n",
    "            pwd = f_data[accessType][\"pwd\"]\n",
    "            driver = f_data[accessType][\"dialect_driver\"]\n",
    "            port = f_data[accessType][\"port\"]\n",
    "\n",
    "            if accessType == \"azureAccess\":\n",
    "                if dialect in f_data[accessType][\"list_workingDialects\"]:\n",
    "                    print (f\"trying the dialect: {dialect}\")\n",
    "\n",
    "                    connection_string = (\n",
    "                        \" Driver={%s}\" %dialect +\n",
    "                        \"; SERVER=%s\" %server + \n",
    "                        \"; Database=%s \" %db + \n",
    "                        \"; UID=%s\" %uid +\n",
    "                        \"; PWD=%s\" %pwd\n",
    "                    )\n",
    "                    \n",
    "                    quoted = urllib.parse.quote_plus(connection_string)\n",
    "                    quoted = f_data[accessType][\"dialect_driver\"] + quoted\n",
    "                    #engine = create_engine(quoted, fast_executemany=True).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "                    engine = create_engine(quoted, fast_executemany=True)\n",
    "                    print (f\"engine created with dialect = {dialect}\")\n",
    "                    try:\n",
    "                        with engine.begin() as conn:\n",
    "                            df = pd.DataFrame([1], columns = ['test'])\n",
    "                            df.to_sql(\"connectionTestTable\", conn, if_exists=\"replace\", index = False)\n",
    "                            print(f\"engine test sucessful\")\n",
    "                            break\n",
    "                    except:\n",
    "                        print(f\"the dialect = {dialect} didn't work\")\n",
    "            else:\n",
    "                quoted = driver + uid + \":\" + pwd + \"@\" + server + \":\" + str(port) + \"/\" + db\n",
    "                engine = create_engine(quoted).execution_options(isolation_level=\"AUTOCOMMIT\")\n",
    "            str_error = None\n",
    "\n",
    "        except:\n",
    "            print('exception found, trying other dialect')\n",
    "            pass\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get response from API\n",
    "def setupAPIrequest(utilities):\n",
    "    schemeHTTP = utilities[\"schemeHTTP\"]\n",
    "    baseHTTP = utilities[\"baseHTTP\"]\n",
    "    extraHTTP = utilities[\"extraHTTP\"]\n",
    "    headers = utilities[\"headers\"]\n",
    "    \n",
    "    #adds default headers\n",
    "    headers['Accept'] =  \"application/json\"\n",
    "    headers['Content-Type'] =  \"application/json\"   \n",
    "\n",
    "    completeHTTP = schemeHTTP + baseHTTP + extraHTTP\n",
    "\n",
    "    if utilities[\"method\"] == \"get\":\n",
    "        response = requests.get(completeHTTP, headers=headers)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorHandle(errSeverity, errProcedure, errDescription, file, engine_azure):\n",
    "    '''\n",
    "    Handles error for logging in AzureDB:\n",
    "    errLocation should be: where is running, application that is running + file name, other info\n",
    "    errDescription should be: what went wrong probably\n",
    "    errProcedure should be: how to restart/check the schedule or other info + if it's ok to retry anytime\n",
    "    errSeverity: 1 to 5, where 1 is wait for next try and 5 is check immediately\n",
    "    the connection is the connection for the AzureDB\n",
    "    '''\n",
    "    print(\"started errorHandle\")\n",
    "    errLocation = globals()[\"util\"][file][\"runLocation\"]\n",
    "    errRunFileName = globals()[\"util\"][file][\"runFileName\"]\n",
    "    errRetry = globals()[\"util\"][file][\"retryOption\"]\n",
    "\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_errorLogTable (errorDescription, errorProcedure, errorStartTime, errorLocation, errorRetry, errorDuration, errorSeverity)\n",
    "        VALUES ('{errDescription}', '{errProcedure}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{errLocation}: {errRunFileName}', '{errRetry}', {timeDifference.total_seconds()}, {errSeverity}) \n",
    "    \"\"\"\n",
    "    #tabela = Table('nfo_errorLogTable', MetaData(), autoload_with=engine_azure)\n",
    "    #query = sa.insert(tabela).values(errorDescription = errDescription, errorProcedure = errProcedure, errorTime = datetime.now().strftime(\"%d/%m/%Y, %H:%M\"), errorLocation = errLocation, errorSeverity = errSeverity)\n",
    "    \n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def successHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the Success outputTable in AzureDB -> wms_vEstoqueConsultaSuccess\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started successHandle\")\n",
    "    runFile = globals()[\"util\"][file][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"resultSuccessTable\"]\n",
    "    runLocation = globals()[\"util\"][file][\"runLocation\"]\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_successRunTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runQueryName}', '{runInputLocation}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber}, {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()} ,'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionHandle(file, additionalInfo, runRowNumber, engine_azure):\n",
    "    '''\n",
    "    Input information on function run success in AzureDB:\n",
    "    :runFile: varchar(100) - describes the filename -> wms_function_vEstoqueConsulta.py\n",
    "    :runStartTime: datetime - describes the startTime \n",
    "    :runQueryName: varchar(100) - describes the queryName -> vEstoqueConsulta\n",
    "    :runInputLocation: varchar(100) - describes the location of the input -> WMS_API\n",
    "    :runOutputTable: varchar(100) - describes the attention outputTable in AzureDB -> wms_vEstoqueConsultaAttention\n",
    "    :runLocation: varchar(100) - describes where the pipeline is running -> AWS_batch\n",
    "    :runDuration: datetime(100) - describes the run duration in seconds\n",
    "    :additionalInfo: varchar(100) - additional information, optional\n",
    "    :runRowNumber: (bigint) - describes how many rows were inserted in the table\n",
    "    :engine_azure: is the azureDB defined engine\n",
    "    '''\n",
    "    print(\"started attentionhandle\")\n",
    "    runFile = globals()[\"util\"][file][\"runFileName\"]\n",
    "    runQueryName = globals()[\"util\"][file][\"runQueryName\"]\n",
    "    runInputLocation = globals()[\"util\"][file][\"runInputLocation\"]\n",
    "    runOutputTable = globals()[\"util\"][file][\"resultSuccessTable\"]\n",
    "    runLocation = globals()[\"util\"][file][\"runLocation\"]\n",
    "    timeDifference = (globals()['endTime'] - globals()['startTime'])\n",
    "    mainInsertionTimeDifference = (globals()['mainEndTime'] - globals()['mainInsertTime'])\n",
    "    attentionInsertionTimeDifference = (globals()['attentionEndTime'] - globals()['attentionInsertTime'])\n",
    "    sql_text = f\"\"\"\n",
    "        INSERT INTO nfo_attentionTable (runFile, runStartTime, runQueryName, runInputLocation, runOutputTable, runLocation, runDuration, runRowNumber, mainInsertionTimeDifference, attentionInsertionTimeDifference, additionalInfo)\n",
    "        VALUES ('{runFile}', '{globals()['startTime'].strftime(\"%m/%d/%Y %H:%M\")}', '{runInputLocation}', '{runQueryName}', '{runOutputTable}', '{runLocation}', '{timeDifference.total_seconds()}', {runRowNumber} , {mainInsertionTimeDifference.total_seconds()}, {attentionInsertionTimeDifference.total_seconds()},'{additionalInfo}') \n",
    "    \"\"\"\n",
    "    with engine_azure.begin() as conn:\n",
    "        conn.execute(sql_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fCorrectDates(dataFrame, dateColumns_dict, list_dfAttention):\n",
    "    '''\n",
    "    gets a normalized data frame and a list of columns in a dictionary to change column type on the dataFrame\n",
    "    returns a list_dfAttention a list with datetime errors, dataframe with the altered columns \n",
    "    '''\n",
    "    for column in dataFrame:\n",
    "        for key, value in dateColumns_dict.items():\n",
    "            if column == key:\n",
    "                #copy the df to errDataTime\n",
    "                errDatetime = dataFrame\n",
    "                #remove empty column cells\n",
    "                errDatetime = errDatetime[errDatetime[column].astype(bool)]\n",
    "                #reindex the errDateTime to match with mask\n",
    "                errDatetime.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                #create a mask where the convertion to datetime fails\n",
    "                mask = pd.to_datetime(errDatetime[column], format=value, errors='coerce').isna()\n",
    "                \n",
    "                #apply to df the mask from the substitution\n",
    "                errDatetime = errDatetime[mask]\n",
    "\n",
    "                #reindex the errDatetime\n",
    "                errDatetime.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                #append dataframe to be concatenated after only if there is > 1 row in the df\n",
    "                if len(errDatetime) > 0:\n",
    "                    list_dfAttention.append(errDatetime)\n",
    "\n",
    "                #the main Dataframe is kept with all the data (and the errors are coerced)\n",
    "                dataFrame[column] = pd.to_datetime(dataFrame[column], format=value, errors=\"coerce\")\n",
    "    \n",
    "    return dataFrame, list_dfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    #open auth file, auth for wms is already in utilities file\n",
    "    auth = open('auth.json')\n",
    "    auth_load = json.load(auth)\n",
    "    \n",
    "    #create AzureDB connection\n",
    "    engine_azure = getConnforMYSQL(auth_load, \"azureAccess\")\n",
    "    conn_azure = engine_azure.connect()\n",
    "\n",
    "    #get Wms content\n",
    "    util = open('utilities.json')\n",
    "    utilities_load = json.load(util)\n",
    "    globals()['util'] = utilities_load\n",
    "\n",
    "    \n",
    "    #wait for good response\n",
    "    good_response = False\n",
    "    #repeat for 503 (server out of resources) and 504 (server request timeout) responses\n",
    "    while good_response == False:\n",
    "        #request the response from the API\n",
    "        response = setupAPIrequest(utilities_load[file])\n",
    "        print('API response: %s' %response.status_code)\n",
    "\n",
    "        if response.status_code != 504 and response.status_code != 503:\n",
    "            good_response = True\n",
    "\n",
    "    #setup global variable for the outcome of the connection\n",
    "    globals()['output'] = \"Failed\"\n",
    "\n",
    "    #setup a dataframe for the attention needed rows\n",
    "    df_attention = pd.DataFrame()\n",
    "    list_dfAttention = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            #start cleaning of data\n",
    "            df = pd.json_normalize(response.json())\n",
    "            \n",
    "            df, list_dfAttention = fCorrectDates(df, utilities_load[file]['dateColumns'], list_dfAttention)\n",
    "\n",
    "            #include the rows in a sigle dataframe then remove duplicates\n",
    "            if len(list_dfAttention) > 0:\n",
    "                df_attention = pd.concat(list_dfAttention,  ignore_index=True)\n",
    "                df_attention = df_attention.drop_duplicates()\n",
    "                #adds the dataconsulta column for historic registration of the error\n",
    "                df_attention['dataconsulta'] = pd.Timestamp.today().strftime('%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "            #prepare df's to upload to AzureDB\n",
    "            df_attention = df_attention.fillna(\"\")\n",
    "            df = df.fillna(\"\")\n",
    "\n",
    "            try:\n",
    "                #tsql_chunksize = 2097 // len(df.columns)\n",
    "                #tsql_chunksize = 1000 if tsql_chunksize > 1000 else tsql_chunksize\n",
    "                #df.to_sql(utilities_load[file][\"resultSuccessTable\"], engine_azure, method='multi', chunksize=tsql_chunksize, if_exists='replace', index=False)\n",
    "                #best result for this process was using only fastexecutemany = true instead of multi + chunksize or any combination of that\n",
    "\n",
    "                #insert into AzureDB the main df\n",
    "                print (f'{file} starting mainInsertion time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "                globals()['mainInsertTime'] = datetime.now()\n",
    "                df.to_sql(utilities_load[file][\"resultSuccessTable\"], engine_azure, if_exists='replace', index=False)\n",
    "                globals()['mainEndTime'] = datetime.now()\n",
    "                \n",
    "                #mark clocks\n",
    "                globals()['endTime'] = datetime.now()\n",
    "                globals()['attentionInsertTime'] = datetime.now()\n",
    "                globals()['attentionEndTime'] = datetime.now()\n",
    "\n",
    "                #if something needs attention\n",
    "                if len(df_attention) > 0 :\n",
    "                    #insert into AzureDB the attention df\n",
    "                    print (f'{file} starting attentionInsertion time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "                    globals()['attentionInsertTime'] = datetime.now()\n",
    "                    df_attention.to_sql(utilities_load[file][\"resultAttentionTable\"], engine_azure, if_exists='append', index=False)\n",
    "                    globals()['attentionEndTime'] = datetime.now()\n",
    "                    #for the rows with date conversion errors or other filtered/coerced rows\n",
    "                    attentionHandle(file, additionalInfo= \"\", runRowNumber= len(df_attention), engine_azure= engine_azure)\n",
    "\n",
    "                #for the main DataFrame\n",
    "                successHandle(file, additionalInfo= \"\", runRowNumber= len(df), engine_azure= engine_azure)\n",
    "                globals()['output'] = \"Success\"\n",
    "            except:\n",
    "                globals()['endTime'] = datetime.now()\n",
    "                errorSuggestedProcedureInsertAzureDB = utilities_load[file][\"errorSuggestedProcedureInsertAzureDB\"]\n",
    "                errorDescriptionInsertAzureDB= utilities_load[file][\"errorDescriptionInsertAzureDB\"]\n",
    "                errorHandle(2, errorDescriptionInsertAzureDB, errorSuggestedProcedureInsertAzureDB, file, engine_azure)\n",
    "        except:\n",
    "            globals()['endTime'] = datetime.now()\n",
    "            errorSuggestedProcedureGeneric = utilities_load[file][\"errorSuggestedProcedureGeneric\"]\n",
    "            errorDescriptionGeneric= utilities_load[file][\"errorDescriptionGeneric\"]\n",
    "            errorHandle(2, errorDescriptionGeneric, errorSuggestedProcedureGeneric, file, engine_azure)\n",
    "    else:\n",
    "        globals()['endTime'] = datetime.now()\n",
    "        errorSuggestedProcedureGeneric = utilities_load[file][\"errorSuggestedProcedureGeneric\"]\n",
    "        errorDescriptionGeneric = utilities_load[file][\"errorDescriptionGeneric\"]\n",
    "        errorHandle(1,  \"External API response code: %s\" %response.status_code, errorSuggestedProcedureGeneric, file, engine_azure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRowNumber():\n",
    "    '''\n",
    "    to test the output of the function, first update example.json file in folder -> to do this, copy the output of the postman response in the dictionary\n",
    "    then run this function and compare the number of rows between AzureDB and the dataframe\n",
    "    '''\n",
    "    result = open(\"example.json\", 'r', encoding='utf-8')\n",
    "    result = json.loads(result.read())\n",
    "\n",
    "    df = pd.json_normalize(result['x'])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wms_function_vRecebimento start time: 16/08/2023 18:38:06\n",
      "trying the dialect: ODBC Driver 18 for SQL Server\n",
      "engine created with dialect = ODBC Driver 18 for SQL Server\n",
      "engine test sucessful\n",
      "API response: 200\n",
      "wms_function_vRecebimento starting mainInsertion time: 16/08/2023 18:38:15\n",
      "started successHandle\n",
      "wms_function_vRecebimento: done with the output: Success, runtime 51.30529\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file = \"wms_function_vRecebimento\"\n",
    "    print (f'{file} start time: {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}')\n",
    "    globals()['startTime'] = datetime.now()\n",
    "    \n",
    "    main(file)\n",
    "\n",
    "    print('%s: done with the output: %s, runtime %s' %(file, globals()['output'], (globals()['endTime'] - globals()['startTime']).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
